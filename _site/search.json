[
  {
    "objectID": "posts/models-overview/models.html",
    "href": "posts/models-overview/models.html",
    "title": "cap5765 notes",
    "section": "",
    "text": "will collect and highlight"
  },
  {
    "objectID": "posts/week-3/wk3-deeper-into-reg.html",
    "href": "posts/week-3/wk3-deeper-into-reg.html",
    "title": "wk 3: deeper into regression",
    "section": "",
    "text": "We understand linear regression, but we left with the FEAR OF EXPONENTIAL GROWTH given to us‚Ä¶as we get into mutliple regression, we have to compare \\(2^P\\) (P being the number of variables) models.\n\n\nComputer least squares fit for all possible subsets then choose between them. If we have 40 variables, this is computing BILLIONS OF MODELS. This is really expensive and it makes things unnecessarily complex.\n\n\n\n\n\n\nTip\n\n\n\nAlgorithms can be GREEDY. Greedy algortihms pick the best immediate output, but does not consider the big picture.\n\n\nFor forward selection:\nstep 1 start with null subset, ie, \\(y = \\beta_0\\)\nstep 2 For \\(K = 1, ... , p\\) fit all (p k) models that contain exactly k predictors\nK = 1:\n\\(M_1: t = \\beta_1 + \\beta_1X_1\\) \\(M_2: t = \\beta_2 + \\beta_2X_2\\) ‚Ä¶ \\(M_{10}: t = \\beta_{10} + \\beta_{10}X_{10}\\)\nstep 3 select single best method among M_0\n\n\n\nBackwards starts with all variables, remove variable with largest p-value\nBest subset https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856\n(what about racing?)\nwhen we are using stepwise, we want to minimize training set MSE. sure, but what about the MSE for the test set?\n\n\n\n\n\n\nNote\n\n\n\nThe MSE for a test set is quadratic, where the MSE for a training set is negative linear.\n\n\nTraining MSE is generally an underestimate of the test MSE, where \\(MSE = RSS/n\\) In fitting a model to the training data using least squares: - regression coefficients are estimated so that RSS is as small as possible - training sets for RSS and \\(R^2\\) cannot be used\n\n\n\nIRL, stepwise is not the best to use.\nPulling out the highlights from the link:\n\n\nIt yields R-squared values that are badly biased to be high.\nThe F and chi-squared tests quoted next to each variable on the printout do not have the claimed distribution.\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]).\nIt has severe problems in the presence of collinearity.\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\nIt allows us to not think about the problem.\nIt uses a lot of paper.\n\n\n\n\n\nBig things to remember here: \\(C_p\\), \\(AIC\\), \\(BIC\\), and \\(R^2\\).\n\n\\(C_p\\)Akaike information criterion (AIC)Bayesian information criterion (BIC)Adjusted \\(R^2\\)\n\n\n\\(C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\\)\n\nd is number of predictors\n\\(\\sigma\\) is estimate of the variance of error for each response\n\\(2d\\hat{\\sigma}^2\\) is the penalty. penalty increase as number of predictors increase\n\nif \\(\\hat{\\sigma}^2\\) is unbiased estimate, then \\(C_p\\) is an unbiased estimate of test MSE\nwhat does bias mean? data bias? model bias? anything that is not a normal distribution?\nSmaller is better.\n\n\n\\(AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2)\\) which simplifies to \\(AIC = \\frac{C_p}{\\hat{\\sigma}^2}\\)\n\ndirectly proportional to \\(C_p\\)\n\nSmaller is better.\n\n\n\\(BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\\)\npenalty is \\(log(n)d\\hat{\\sigma}^2)\\)\nbecuase it is using log, BIC penalizes large models more than AIC, which always uses the penalty of 2 (this is bc log(7))\nSmaller is better.\n\n\n$ R^2 = 1 - \n\nit can be negative, but that means thiings are going very bad. it cannot be greater than 1 (you can‚Äôt explain more than 100% of your model). negative values occur when your model is worse than just always guessing the average of all the data points More info on negative \\(R^2\\)\nMAXIMIZE!\n\n\n\n\n\n\nKinda like subset, but better. You try to make some of the coefficients to zero, aka shrinking them. Use all the predictors. Ridge regression and lasso are the most famous. (Don‚Äôt shrink the intercept, only the predictors.)\n\\[\ny = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\n\\]\nQ: can using a shrinkage method increase variance?\n\n\nFormula: \\[\n\\sum_i(y_i-\\beta_0-\\sum_j\\beta_jX_{ij})^2\n\\]\nRidge is good in terms of bias-variance trade off. As \\(\\lambda\\) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n\\(\\lambda\\) if lambda is 0, the penalty is the same as the RSS. when lambda is greater than 0, RSS is greater.\nminimize this quanity, so each lambda\n\n\\(\\lambda\\sum\\beta_j^2\\) is shrinkage penalty\n\\(\\lambda\\) is tuning parameter\n\\(\\lambda \\ge 0\\)\n\nwhen \\(\\lambda = 0\\), it is just RSS\nThe coefficient estimate obtained by Ridge Regression are denoted \\(\\beta_\\lambda^R\\) for each \\(\\lambda\\)\nall these \\(\\beta_j^R\\) depend on \\(\\lambda\\) (\\(\\beta_j^R\\) <- is a vector when p > 1)\nselecting good value of \\(\\lambda\\) is critical (we will use cross validation) ridge uses the \\(l_2\\) norm is $||\\_2 = ||^2 $\na range of 0/least squares to least squares/least squares (1)\ncon: ridge retains all coefficients, no selection\n\n\n\non a scale of least squares to zero, how are you feeling today?\n\\[\n\\sum_i(y_i-\\beta_0-\\sum_j\\beta_jX_{ij})^2 +\\lambda\\sum_j|\\beta_j|\n\\]\nfunction of data, but also function of \\(\\lambda\\)\nfix \\(\\lambda\\) and then do minimization problem. if \\(\\lambda\\) is near 0, it esentially is just least squared. if the \\(\\lambda\\) approaches infinity, ridge coefficient estimates shrink to zero\nlasso uses \\(l_1\\) norm is $ ||\\_1 = ||$\nlasso does not retain all of the coefficients, so it is in fact a selection method\nOne obvious advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors."
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html",
    "href": "posts/week-2/wk2-cda-linear.html",
    "title": "wk 2: linear regression",
    "section": "",
    "text": "Linear regression is a fabulous place to start with statistical learning. The math is (relatively) simple, but it can feel like there‚Äôs a lot of moving parts if you‚Äôre new to all this. In general, we want to know 3 things this week about linear regression: create the best line possible, assess the coefficients and know if we did a good job.\n\n\n\nThe formula for least squares is:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\nSo we want to calculate out two things: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope).\nHere are the formulas: \\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThis course involves a lot of math, and variables. It‚Äôs hard to keep them straight if you‚Äôre not used to looking at a lot of statistics.\nBars (\\(\\bar{x}\\)) are for means. Think of üòê being a mean face.\nHats (\\(\\hat{x}\\)) are for estimated numbers. Think of \\(\\hat{y}_0\\) as wearing a hat to cosplay as their best guess of what \\(y_0\\) looks like ü§† (sometimes y is not very good at cosplaying).\n\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so let‚Äôs not be too concerned with this right now.\n\n\n\nRounding up a few key points:\n\nThe difference between \\(\\hat{y}_i\\) and \\(y_i\\) is \\(e_1\\). This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nStandard error has a reverse relation with number of training set.\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n\n\nResidual Standard Error = \\(\\sqrt{-\\frac{RSS}{n-2}}\\)\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular ol‚Äô residuals. We also need to divide by something to account for the degrees of freedom, thus \\(n-2\\). Interested in why we divide by 2? (tldr; residual degrees of freedom.)\nCONFIDENCE INTERVAL \\(\\hat{B_1} \\pm 2SE(\\hat{B_1})\\)\nWhat is a confidence interval tho? values that with 95% probability, the range will contain the true unknown value\nHere‚Äôs a thread to help understand:\n\n\nHey Stats folk, what's your 280 character definition of a confidence interval?ü§î\n\n‚Äî Chelsea Parlett-Pelleriti (@ChelseaParlett) March 13, 2018\n\n\n\n\n\n\n\n\nModels are toddlers. They LOVE saying NO. You might say, ‚ÄúThese coefficents mean something!‚Äù ‚ÄúNO‚Äù. While you can win toddlers (and college students) over with mac and cheese, models need cold, hard facts.\n\\(H_0:\\) There is no relation between X and Y, that is \\(H_0:\\beta_1=0\\) \\(H_A:\\) There is some relationship between X and Y, that is \\(H_A:\\beta_1\\ne0\\)\nif \\(\\beta_1 = 0\\),\nto test null, we do a \\(t\\) statistic:\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nwhat is a p-value?\n\n\na small p-value is good‚Äì it means our model is accurate\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\nhaving a large p-value‚Äì less likely there is a relation between X and Y\n\n\n\n\nRSE\n\\(R^2\\)\nF-statistic\n\nRSE is used to either estimate the STANDARD ERROR of \\(\\beta_x\\) OR the accuracy of the overall model\nWe already know RSS and \\(TSS = \\sum_{i=1}^n(y_i-\\bar{y})\\).\n$ R^2 = = 1 - $ for how much variance in Y is explained by X. As a model‚Äôs RSS shrinks, \\(R^2\\) will get closer and closer to 1.\n\n\n\n\n\n\nTip\n\n\n\nA sanity check here: \\(R^2\\) should always be [0,1]. If your \\(R^2\\) is bigger than 1 or larger than 0‚Ä¶something is going very wrong.\n\n\nIf: \\[\nTSS = \\sum_{i=1}^n((y-\\bar{y})+(\\hat{y}_i-\\bar{y}))^2\n\\]\nwhy is? \\[\n\\sum_{i=1}^n(y_i-\\hat{y_i})(\\hat{y}_i-\\bar{y_i}) = 0\n\\]\nF statistic: If you get a large f value (one that is bigger than the F critical value found in a table), it means something is significant. In general, if F > 1, you will reject the null hypothesis. If F<1, coefficient is nonzero.\n\n\n\nAre at least one predictor \\(X_1, X_2, ... X_p\\) useful in predicting? (ie, p-value<0.05 and nonzero. Okay, so the coefficient is how much an input changes. If something has no effect, the coefficient will be zero (or very close to it))\nDo all the predictors help to explain Y, or just a few? (reminder, there‚Äôs \\(2^P\\) subsets‚Ä¶ we‚Äôre not going to check all of them.)\nHow well does the model fit?\nGiven a\n\n\n\nInterpret \\(\\beta_j\\) as the average effect on Y as a one unit increase. Hold all other variables constant.\n\n\n\nwhat to do with insignificant p-values?\n\n\n\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "posts/week-9/lda-qda-knn.html",
    "href": "posts/week-9/lda-qda-knn.html",
    "title": "wk 9: knn + pca",
    "section": "",
    "text": "KNN\nknn is literally looking at a data point and peeking at the classifcations of its neighbors, then making a guess. ie, \\(N_0\\) = {1,2,3} where {1,2,3} are the 3 closest points by euclidean distance\nthe proper algorithm is:\n\\(Pr(Y=k|X=x_0)=\\frac{1}{K}\\sum_{i\\in N_0}I(y_i=k)\\) if 1=red, 2=red, 3=yellow, then \\(Pr=\\frac{2}{3}\\)\nwhat happens if neighbor only has one point?\nwhat if no neighbors? only goes off itself, if 1 neighbor\nk is usually odd to avoid ties\nits not really modeling, it just queries training data for new points\n\n\nwhat is the black box model?\nyou can‚Äôt see the inner workings of a model when you are using it in code‚Ä¶even models that are really not considered to be black box (ie, linear regression) can act as black boxes in practice.\n\n\nPCA\nmain goals: identify pattern in data\ndetect correlation btwn variables\nstrong correlation-> tries to reduce the dimensionality\nfinding the maximum variance in high dimensionality data and project to a smaller subspace while retaining most of the info\nsummary of PCA:\n\nstandardize/normalize the data\nobtain the eigenvector eigenspaces from covariance matrix\nperform singular value decomposition (SVD)\nsort eigenvalues in decreasing order and choose k largest eigenvalues, where k is # of dimensions of new feature subspaces\ntransform the original data set X variables to obtain feature subspace Y\n\nto understand after: largest eigenvalues is largest variety, explain variance of data in the new features (this is kinda like \\(R^2\\)?)\n\n\n\n\n\n\nTip\n\n\n\nwhat is the difference btwn correlation and covariance? they are the same, but correlation is scaled\n\n\nreduce dimensions and create fewer ones allow for viz many dimensions in 2 dim, or preprocessing for models that cant take high dimensions\nends up being a normalized linear combination of features.\nfirst principal component of a set of features \\(X_1, X_2, ..., X_p\\) is the normalized linear combination of the features\n\\[\nZ_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + ... + \\phi_{p1}X_p\n\\]\nthat has the largest variance\nWe mean that normalized that \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\).\nwe refer to \\(\\phi_{11}, ..., \\phi_{p1}\\) as the loadings of the first principal component.\nAnd think of them as the loading vector \\(\\phi_1\\)\nAssuming we have a \\(n \\times p\\) data set \\(\\mathbf{X}\\)\nsince we are only interested in the variance we assume that the variables have been centered\n\\[\n\\underset{\\phi_{11}, ..., \\phi_{p1}}{\\text{maximize}} \\left\\{ \\dfrac{1}{n} \\sum^n_{i=j} \\left( \\sum^p_{j=1} \\phi_{j1}x_{ij} \\right)^2 \\right\\} \\quad \\text{subject to} \\quad \\sum_{j=1}^p  \\phi_{j1}^2 = 1\n\\]\n(m<p) still thinking about \\(R^2\\)‚Äì so would you be able to use this? if PCA is essentially trying to figure out how to capture variance and is represented by explained variance, and \\(R^2\\) is a measure of explained variance‚Ä¶\nPCR -> principal component analysis using regression\n$ y = _0 + _1Z_1 + _2Z_2$ is the linear regression using PCA components \\(Z_1\\) and \\(Z_2\\)\ntldr; create new variables that are linear combinations of original variables linear combinations are uncorrelated new variables are principal components\nhow to choose number of components?\ngeometric explanation of PCA: you usually center and scale\nthe more correlated the original data, the better this line (first component) will explain the actual values of the observed measurements‚Äìgoal is to minimize sum of residuals (distance from origin to projected point is called the ‚Äúscore‚Äù)\nfind the first latent variable == first principal component\nwould you use PCA and then stop? or would you usually use it as feature engineering and then continue on\nKnowing relation btwn maximizing variance and how this is related to minimize residual\n\nwhat is SVD\nsingular value decomposition use it on covariance matricies, guarantee you find the \\(Z_1\\) has maximum variance, since it has the largest eigenvalue second component \\(Z_2\\) will be orthogonal to \\(Z_1\\), so it will be second largest variance and NOT CORRELATED with \\(Z_1\\) also reducing the dimension\n\n\n\nOn covariance\ncorrelation matrix is between -1 and 1. covariance any number.\ncovariance is practical esp w mulitvariate issues.\nvariance: each value subtracted from mean of that variable, differences squared, divided by number of values in that variable\nso COvariance is calculated between different variables: values of both variables are multiplied by taking the difference from the mean and diving by number of values cov(x,x) = var(x)\n\npositive: positive relationship, variate in the same direction\nnegative: negative relationship between 2 variables, variate in opposite directions\nzero: no relation between data\nsize of a covariance value: not scaled, so just bc numbers are high doesn‚Äôt mean that the covariance is high\n\nA <- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), 3, 3, byrow = TRUE)\n\n\nev <- eigen(A)\nvalues <- ev$values\n\n\nev$vectors\n\n           [,1]       [,2]      [,3]\n[1,]  0.7453560  0.6666667 0.0000000\n[2,] -0.5962848  0.6666667 0.4472136\n[3,]  0.2981424 -0.3333333 0.8944272\n\n\neigenvalues and eigenvectors of covariance matrix eigenvalues find magnitude and spread of data points when cov = 0, eigenvalues directly equal to variance values eigenvectors show direction\nuse these in PCA and LDA :D\nquiz on PCA + covariance not coding"
  },
  {
    "objectID": "posts/week-7/week-7.html",
    "href": "posts/week-7/week-7.html",
    "title": "wk 7 recap: midterm study guide",
    "section": "",
    "text": "Apologies for this week‚Ä¶its a little messy! Scratch notes for midterm review"
  },
  {
    "objectID": "posts/week-7/week-7.html#relation-between-x-and-y-in-linear-regression-vs.-logistic-regression",
    "href": "posts/week-7/week-7.html#relation-between-x-and-y-in-linear-regression-vs.-logistic-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Relation between X and Y in linear regression vs.¬†logistic regression",
    "text": "Relation between X and Y in linear regression vs.¬†logistic regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-linear-regression",
    "href": "posts/week-7/week-7.html#explain-linear-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain linear regression",
    "text": "Explain linear regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#brief-explanation-of-linear-regression",
    "href": "posts/week-7/week-7.html#brief-explanation-of-linear-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Brief explanation of linear regression",
    "text": "Brief explanation of linear regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#explain",
    "href": "posts/week-7/week-7.html#explain",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain:",
    "text": "Explain:\n\nBest subset selection\nStepwise selection\n\nForwards: start w 0, add in predictors\nBackwards: start w all, subtract predictors\nWhich is better?\n\nBest subset\n\nWhy?\n\nFW/BW: stepwise will bias R^2 values, model will smallest training RSS will be best subset, since it will encompass all the options from forward and backward stepwise, as well as interactions between these variables"
  },
  {
    "objectID": "posts/week-7/week-7.html#difference-between-ridge-and-lasso",
    "href": "posts/week-7/week-7.html#difference-between-ridge-and-lasso",
    "title": "wk 7 recap: midterm study guide",
    "section": "Difference between ridge and LASSO",
    "text": "Difference between ridge and LASSO\nComputing penalty Ridge \\(B_j^2\\), makes a O when you draw contours for error and constraint functions LASSO \\(|B_j|\\), makes a square when you draw contours for error and constraint functions Larger tuning parameter LASSO can be selection Think it can ‚Äúlasso‚Äù a portion of the variables Ridge all the predictors will remain in final model"
  },
  {
    "objectID": "posts/week-7/week-7.html#tf-lasso-and-ridge-compared-to-least-squares-are-less-flexible-and-will-give-improved-prediction-accuracy-when-its-increase-in-bias-is-less-than-its-decrease-in-variance.",
    "href": "posts/week-7/week-7.html#tf-lasso-and-ridge-compared-to-least-squares-are-less-flexible-and-will-give-improved-prediction-accuracy-when-its-increase-in-bias-is-less-than-its-decrease-in-variance.",
    "title": "wk 7 recap: midterm study guide",
    "section": "T/F: Lasso and Ridge, compared to least squares, are less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance.",
    "text": "T/F: Lasso and Ridge, compared to least squares, are less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nbias-variance trade-off. ls have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias. This consequently can generate more accurate predictions. In addition, lasso performs variable selection which makes it easier to interpret than other methods like ridge regression."
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-logistic-regression",
    "href": "posts/week-7/week-7.html#explain-logistic-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain logistic regression:",
    "text": "Explain logistic regression:\nLog odds of linear regression Coefficients found by maximum likelihood"
  },
  {
    "objectID": "posts/week-7/week-7.html#whether-a-student-will-passfail-in-an-exam-based-on-hours-of-study-can-be-solved-with-logisticlinear-regression.",
    "href": "posts/week-7/week-7.html#whether-a-student-will-passfail-in-an-exam-based-on-hours-of-study-can-be-solved-with-logisticlinear-regression.",
    "title": "wk 7 recap: midterm study guide",
    "section": "Whether a student will pass/fail in an exam based on hours of study can be solved with LOGISTIC/LINEAR regression.",
    "text": "Whether a student will pass/fail in an exam based on hours of study can be solved with LOGISTIC/LINEAR regression.\nWhy? Output variable is a classification, not a regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-lda",
    "href": "posts/week-7/week-7.html#explain-lda",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain LDA",
    "text": "Explain LDA\nDim reduction Modeling distribution of predictors Like PCA that it reduces dimensions, but focuses on SEPERABILITY among known categories Creates a new acis and projects the data on this axis that maximizes separation 2 criteria, considered simultaneously: maximize distance between means (if high dim, use centroid), minimize variation w/in cat"
  },
  {
    "objectID": "posts/week-7/week-7.html#lda-assumptions",
    "href": "posts/week-7/week-7.html#lda-assumptions",
    "title": "wk 7 recap: midterm study guide",
    "section": "LDA assumptions",
    "text": "LDA assumptions\nobservations within each class come from a normal distribution with a class specific mean and a common variance œÉ2, and plugging estimates for these parameters into the Bayes classifier Number of parameters in LDA For LDA, (p+1) parameters are needed to construct the discriminant function in (2). For a problem with K classes, we would only need (K-1) such discriminant functions by arbitrarily choosing one class to be the base class (subtracting the base class likelihood from all other classes). Hence, the total number of estimated parameters for LDA is (K-1)(p+1).\nWhen there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.\nIf the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression."
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-bias-variance-tradeoff",
    "href": "posts/week-7/week-7.html#explain-bias-variance-tradeoff",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain bias-variance tradeoff",
    "text": "Explain bias-variance tradeoff\nInability for a machine learning method to capture true relationship is BIAS (if it cant ever match, it is largely BIASED) (swayed by every single data point, no reservations, no backbone, no pre-determined bias to keep it safe) simple models tend to have higher bias Difference in fits between data sets is called VARIANCE (hard to predict how it will do with future data sets if high variance) complex models tend to have higher variance\nIdeally low bias, low variability"
  },
  {
    "objectID": "posts/week-7/week-7.html#what-is-over-fitting",
    "href": "posts/week-7/week-7.html#what-is-over-fitting",
    "title": "wk 7 recap: midterm study guide",
    "section": "What is over-fitting?",
    "text": "What is over-fitting?\nTraining model to be too flexible (not enough bias) You can reduce this by using cross fold validation\nEXTRAS:\nPCA: Focuses on dimension reduction by most variation\n(find the sweet spot with regularization, boosting, bagging)"
  },
  {
    "objectID": "posts/week-1/wk1-cda-intro.html",
    "href": "posts/week-1/wk1-cda-intro.html",
    "title": "wk 1: vocab and linear algebra",
    "section": "",
    "text": "‚ÄúA good model is one that accurately predicts the outcome.‚Äù (sometimes)\nLearning primer:\nOn types of data:\n- Unstructured data does not follow any rhyme or reason\n- Semi-structured has some consitency, but no precise structure\n- Structured data has persistent order\nOn model outputs:\n- Quantitative data can be measured (numeric)\n- Qualitative data is descriptive (categorical) but qualitatative\nOn types of model:\n- Supervised learning is labeled data\n    - Some issues in supervised learning could be overfitting\n- Unsupervised learning is unlabeled data\n    - Some issues could be know\nmatricies: rectangular array, size (row, column) special <3 for SQUARE (n,n) matricies\n\\[\nA=[a_{ij}]\n\\] (the entry on row i and column j of matrix A)\nif \\(A\\) and \\(B\\) are the same size,\n\\(A + B\\)\n\\(B - B\\)\n\\(cA + rB\\) <- this is also called a linear combination of A and B\n\\[A = \\begin{bmatrix}\n1 & -1 & 1\\\\\n0 & 2 & 4\n\\end{bmatrix}\n\\]\nIf you‚Äôre like me and immediately think ‚Äúoh yeah, I‚Äôm going to mae a computer do that,‚Äù be forewarned‚Äìdot products do not work as expected out of the box in base Python OR base R! :)\nPython:\n\n# python\n\nA = [[1,-1,1],[0,2,4]]\nB = [[2,-10,7], [3,8,6]]\n\nA+2*B # okay, this actually doesn't work\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\nR:\n\n# R\n\nmatrix1 <- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)\nmatrix2 <- matrix(c(5, 2, 0, 9, 3, 4), nrow = 3)\n\nmatrix1*matrix2 # OKAY THIS DOESNT WORK EITHER\n\nError in matrix1 * matrix2 : non-conformable arrays\nSo neither of these languages support matrix multiplication the way you multiply numbers. How do we do this?\nPython:\n\nA = [[1,-1,1],[0,2,4]]\nB = [[2,-10,7], [3,8,6]]\n\nA+2*B # okay, this actually doesn't work\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\nR:\n\nmatrix1 <- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)\nmatrix2 <- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)\n\nmatrix1*matrix2 # OKAY THIS DOESNT WORK EITHER\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\n\nMatrix multiplication\ndot product result is ALWAYS A NUMBER - use dot product to create matrix multiplication\nNotation:\n\nQuantitative output: Y\nQualitative output: G\nMatrix: A\n\nI wanted to add more on how to do matrix multiplication and such, but I will never do as good of a job as https://e2eml.school/transformers.html\n\n\n\nLinear Models:\n- you would use regression for numerical\n- you would use classification for \n\\[\ny = mx+b\n\\]\nwhere x = input y = output w = \\(\\begin{bmatrix} m \\\\ b \\end{bmatrix}\\)\ngoal is to make the line of best fit of the weights\nGiven input vector \\(X^T = (X_1, X_2, ..., X_p)\\) we predict \\(Y\\) by \\[\n\\hat{Y}= \\hat{B} + (X_1\\hat{B_1}+ X_1\\hat{B_2} + ... + X_p\\hat{B_p})\n\\] \\[\n\\hat{Y}= \\sum_{j=1}^PX_j\\hat{B_j}\n\\] where $ $ is the intercept.\nIf you have a vector \\(X=\\begin{bmatrix}X_1 \\\\ X_2 \\\\ ... \\\\ X_p\\end{bmatrix}\\)\nlinear model is \\[\nf(x) = X^T\\hat{B}\n\\]\nbut many different techniques to have flavors of linear modesl\n\\[\nError = y - \\hat{Y}\n\\]\nHow to tell if your model is doing well:\nResidual sum square of errors and least square we pick coefficients (weights), \\(\\hat{B}\\) to minimize the RSS\n\\[\nRSS(\\hat{B})= \\sum_{j=1}^N(y_j-x_j^T\\hat{B})^2\n\\]\nSimple linear regression:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown\n\\[\n\\hat{y}=\\hat{B_0} + \\hat{B_1}x\n\\]\n\\[\n\\hat{y_i}=\\hat{B_0} + \\hat{B_1}x_i\n\\]\n(is the prediction of Y based on x_i)\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nwhere \\(y_i\\) is actual and \\(\\hat{y_i}\\) is predicted\n\n\nThe least sqaure is\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]"
  },
  {
    "objectID": "posts/week-10/bootstrapping.html",
    "href": "posts/week-10/bootstrapping.html",
    "title": "wk 10: bootstrapping",
    "section": "",
    "text": "Resampling\nResampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.\nGives distribution of sample population\ncommon ways to\n\n\nBootstrap\nThe bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method\nbootstrapping was originally a method for assessing statistical accuracy of an estimator\n\nfirst, each bootstrap sample must be of the same size as the original sample second, each bookstrap sample must be taken with replacement from the original sample\nis bootstrapping considered simulated data? no‚Äì simulated would be new people. this is reusing people\nlinear function\nBeer = \\(\\alpha\\) + \\(\\beta\\)(wings) | n=30\n\ntake sample n=30\ndo regression to find \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\nkeep track of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\ndo this 10,000 times\nplot distribution\n\nwe end up with a bootstrap distribution for \\(\\hat{\\beta}\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cap5765 notes",
    "section": "",
    "text": "wk 3: deeper into regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nwk 2: linear regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nwk 9: knn + pca\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nwk 7 recap: midterm study guide\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nwk 1: vocab and linear algebra\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nwk 10: bootstrapping\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "iz",
    "section": "",
    "text": "Just posting my silly little notes about computational data analysis!\n\ncheers,\nisabel zimmerman"
  },
  {
    "objectID": "project/ufo-sightings.html#read-in-data",
    "href": "project/ufo-sightings.html#read-in-data",
    "title": "cap5765 notes",
    "section": "Read in data",
    "text": "Read in data\nLet‚Äôs first set a seed for reproducibility and read in our UFO data.\n\nfrom siuba import *\nfrom siuba.siu import call\nimport pandas as pd\n\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv\"\n)"
  },
  {
    "objectID": "project/ufo-sightings.html#what-does-this-data-look-like",
    "href": "project/ufo-sightings.html#what-does-this-data-look-like",
    "title": "cap5765 notes",
    "section": "What does this data look like?",
    "text": "What does this data look like?\n\nraw.columns\n\n\nraw.head(3)\n\n\nlen(raw)"
  },
  {
    "objectID": "project/ufo-sightings.html#clean-data",
    "href": "project/ufo-sightings.html#clean-data",
    "title": "cap5765 notes",
    "section": "Clean data",
    "text": "Clean data\nWe will need to make sure these columns are all in the type that sklearn will be able to make sense of later, so let‚Äôs start by choosing a few columns to compute on. I chose to just look at UFOs seen in the United States, parsed the date, and set the state and ufo shape as categories. Then, I dropped columns with NA values. We see here the resulting data.\n\nufo_usa = (\n    raw\n    >> select(\n        -_.described_encounter_length, -_.city_area, -_.description, -_.date_documented\n    )\n    >> filter(_.country == \"us\")\n    >> separate(_.date_time, [\"date\", \"time\"], sep=\" \")\n    >> mutate(\n        date=call(pd.to_datetime, _.date),\n        state=_.state.astype(\"category\"),\n        ufo_shape=_.ufo_shape.astype(\"category\"),\n    )\n    >> select(-_.time, -_.country)\n).dropna()\nufo_usa.head(3)"
  },
  {
    "objectID": "project/ufo-sightings.html#shape-distribution",
    "href": "project/ufo-sightings.html#shape-distribution",
    "title": "cap5765 notes",
    "section": "Shape distribution",
    "text": "Shape distribution\nIt might be useful to see how often each shape of UFO occurs. This could be important since different models work better or worse on imbalanced classes. Also, it is important when assessing model metrics. For example, while we might be impressed if a model has 90% accuracy, if the shape ‚Äúoval‚Äù occurs 90% of the time, our model is performing no better than just guessing ‚Äúoval‚Äù all the time.\n\nfrom plotnine import *\n\n(\n    ufo_usa\n    >> group_by(_.ufo_shape)\n    >> count()\n    >> ggplot(aes(\"ufo_shape\", \"n\"))\n    + geom_col()\n    + scale_x_discrete(limits=ufo_usa[\"ufo_shape\"].value_counts().index.tolist()[::-1])\n    + coord_flip()\n    + theme(axis_text_x=element_text(angle=90))\n    + labs(y=\"Count\", x=\"Shape\", title=\"Number of UFOs by Shape\")\n)\n\nThere is a wide variety of when shapes occur, from over 12,500 times to just once or twice."
  },
  {
    "objectID": "project/ufo-sightings.html#grouping-the-other-shape",
    "href": "project/ufo-sightings.html#grouping-the-other-shape",
    "title": "cap5765 notes",
    "section": "Grouping the ‚Äúother‚Äù shape",
    "text": "Grouping the ‚Äúother‚Äù shape\nTo make this a litte easier to classify, let‚Äôs consider anything that occurs less than 2500 times to be the shape ‚Äúother.‚Äù\n\nbottom_shapes = (\n    ufo_usa\n    >> group_by(_.ufo_shape)\n    >> count()\n    >> filter(_.n < 2500)\n    >> mutate(ufo_other=\"other\")\n)\n\njoined_df = full_join(\n    ufo_usa,\n    bottom_shapes,\n)"
  },
  {
    "objectID": "project/ufo-sightings.html#section",
    "href": "project/ufo-sightings.html#section",
    "title": "cap5765 notes",
    "section": "",
    "text": "Then, we will join the custom labeled data frame and original data frame, and drop the now unnecessary columns.\n\nfrom siuba.dply.vector import coalesce\n\nclean_df = (\n    joined_df\n    >> mutate(shape=coalesce(_.ufo_other, _.ufo_shape))\n    >> select(-_.ufo_shape, -_.ufo_other, -_.n)\n)\nclean_df\n\nWe will once again peek at the distribution of UFO shapes.\n\n(\n    clean_df\n    >> group_by(_.shape)\n    >> count()\n    >> ggplot(aes(\"shape\", \"n\"))\n    + geom_col()\n    + scale_x_discrete(limits=clean_df[\"shape\"].value_counts().index.tolist()[::-1])\n    + coord_flip()\n    + theme(axis_text_x=element_text(angle=90))\n    + labs(y=\"Count\", x=\"Shape\", title=\"Number of UFOs by Shape\")\n)"
  },
  {
    "objectID": "project/ufo-sightings.html#modeling-train-test-splitting",
    "href": "project/ufo-sightings.html#modeling-train-test-splitting",
    "title": "cap5765 notes",
    "section": "Modeling: train-test splitting",
    "text": "Modeling: train-test splitting\nWith the data cleaned to our liking, we can move along to modeling. We will first start by breaking up this data into a training (85%) and testing (15%) set, so our model doesn‚Äôt ‚Äúsee‚Äù the answers when we are training it.\n\nfrom sklearn import impute, preprocessing, model_selection\nimport numpy\n\nnumpy.random.seed(500)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    clean_df.drop(columns=\"shape\"), clean_df[\"shape\"], test_size=0.15\n)"
  },
  {
    "objectID": "project/ufo-sightings.html#modeling-preprocessing",
    "href": "project/ufo-sightings.html#modeling-preprocessing",
    "title": "cap5765 notes",
    "section": "Modeling: preprocessing",
    "text": "Modeling: preprocessing\nNext, we want to do some preprocessing. We have a mix of categorical and numerical features."
  },
  {
    "objectID": "project/ufo-sightings.html#section-1",
    "href": "project/ufo-sightings.html#section-1",
    "title": "cap5765 notes",
    "section": "",
    "text": "We will start by making the categorical features (state) machine readable by using an ordinal encoder. This will map each category in a column to an integer (eg. ‚ÄòIL‚Äô = 0, ‚ÄòFL‚Äô = 1, ‚ÄòNY‚Äô = 2, and so on). The downfall of this is that it will create interactions between variables that do not exist (eg. FL*2 does not equal NY). However, if we choose a different encoder, such as a one-hot encoder, our data will become highly dimensional, which may cause weak performance. In training, I experimented with both types of encoders, and the ordinal encoder seemed to perform better, which makes sense due to the lower dimensionality in the data.\nFor the numeric features, we will scale them using a standard scaler. For any missing values, we will use an imputer that will replace the missing value with the mean of the column."
  },
  {
    "objectID": "project/ufo-sightings.html#section-2",
    "href": "project/ufo-sightings.html#section-2",
    "title": "cap5765 notes",
    "section": "",
    "text": "We will put these in a single preprocessor variable, and make a column transformer that will select what preprocessing based on the type of the column.\n\nfrom sklearn import impute, preprocessing, pipeline\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncat_pipe = preprocessing.OrdinalEncoder()\n\nnum_pipe = pipeline.make_pipeline(\n    preprocessing.StandardScaler(), impute.SimpleImputer(strategy=\"mean\")\n)\n\npreprocessor = make_column_transformer(\n    (cat_pipe, selector(dtype_include=\"category\")),\n    (num_pipe, selector(dtype_include=\"number\")),\n    n_jobs=2,\n)"
  },
  {
    "objectID": "project/ufo-sightings.html#modeling-the-model",
    "href": "project/ufo-sightings.html#modeling-the-model",
    "title": "cap5765 notes",
    "section": "Modeling: the model",
    "text": "Modeling: the model\nFinally, we can get to the model itself. We will create a pipeline of the preprocessing step and a random forest classifier. I added extra trees and a warm start to hopefully boost the model‚Äôs performance. A warm start uses the solution of previously fitted models to better fit the next batch of decision trees, rather than training from scratch each time."
  },
  {
    "objectID": "project/ufo-sightings.html#section-3",
    "href": "project/ufo-sightings.html#section-3",
    "title": "cap5765 notes",
    "section": "",
    "text": "from sklearn import ensemble, pipeline\n\nclf = pipeline.Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\n            \"classifier\",\n            ensemble.RandomForestClassifier(n_estimators=100, warm_start=True),\n        ),\n    ]\n)\n\nclf.fit(X_train, y_train)"
  },
  {
    "objectID": "project/ufo-sightings.html#modeling-metrics",
    "href": "project/ufo-sightings.html#modeling-metrics",
    "title": "cap5765 notes",
    "section": "Modeling: metrics",
    "text": "Modeling: metrics\nNow, we get to see how well our model is performing!\n\nfrom sklearn import metrics\n\nclf_report = pd.DataFrame(\n    metrics.classification_report(\n        y_true=y_test, y_pred=clf.predict(X_test), output_dict=True\n    )\n)\nclf_report"
  },
  {
    "objectID": "project/ufo-sightings.html#version-the-model",
    "href": "project/ufo-sightings.html#version-the-model",
    "title": "cap5765 notes",
    "section": "Version the model",
    "text": "Version the model\nWhen experimenting with this model, I had a lot of different iterations! We can use something called model versioning to keep track of each model.\n\nfrom vetiver import VetiverModel, vetiver_pin_write\nimport pins\n\nv = VetiverModel(\n    clf,\n    \"ufo\",\n    ptype_data=X_train,\n    # metadata={\n    #     \"preprocessing\": {\"cat\": [\"ordinal_encoder\"], \"num\": [\"scaler\", \"imputer\"]},\n    #     \"clf_report\": clf_report.to_json(),\n    # },\n)\nboard = pins.board_folder(\".\", allow_pickle_read=True)\nvetiver_pin_write(board, v)"
  },
  {
    "objectID": "project/ufo-sightings.html#section-4",
    "href": "project/ufo-sightings.html#section-4",
    "title": "cap5765 notes",
    "section": "",
    "text": "board.pin_versions(\"ufo\")\n\nWe can even read back previous versions, and see information stored about them."
  },
  {
    "objectID": "project/ufo-sightings.html#read-previous-version",
    "href": "project/ufo-sightings.html#read-previous-version",
    "title": "cap5765 notes",
    "section": "Read previous version",
    "text": "Read previous version\n\nimport vetiver\n\nv2 = VetiverModel.from_pin(board=board, name=\"ufo\", version='20221214T232049Z-a7603')\nv2.metadata"
  },
  {
    "objectID": "project/ufo-sightings.html#launch-api",
    "href": "project/ufo-sightings.html#launch-api",
    "title": "cap5765 notes",
    "section": "Launch API",
    "text": "Launch API\nIf we want to run this model as a service at a local API endpoint, we can do so using vetiver as well.\n\nfrom vetiver import VetiverAPI\n\nVetiverAPI(v2).run()"
  },
  {
    "objectID": "project/ufo-sightings.html#create-dockerfile-and-accessory-files",
    "href": "project/ufo-sightings.html#create-dockerfile-and-accessory-files",
    "title": "cap5765 notes",
    "section": "Create Dockerfile and accessory files",
    "text": "Create Dockerfile and accessory files\nFinally, we can also quickly dockerize this model with helper functions.\n\nvetiver.prepare_docker(board=board, pin_name=\"ufo\")"
  }
]