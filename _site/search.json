[
  {
    "objectID": "posts/CAP5765-week-1/wk1-cda-intro.html",
    "href": "posts/CAP5765-week-1/wk1-cda-intro.html",
    "title": "wk 1: vocab and linear algebra",
    "section": "",
    "text": "Learning primer:\nOn types of data:\n- Unstructured data does not follow any rhyme or reason\n- Semi-structured has some consitency, but no precise structure\n- Structured data has persistent order\nOn model outputs:\n- Quantitative data can be measured (numeric)\n- Qualitative data is descriptive (categorical) but qualitatative\nOn types of model:\n- Supervised learning is labeled data\n    - Some issues in supervised learning could be overfitting\n- Unsupervised learning is unlabeled data\n    - Some issues could be know\nmatricies: rectangular array, size (row, column) special <3 for SQUARE (n,n) matricies\n\\[\nA=[a_{ij}]\n\\]\n(the entry on row i and column j of matrix A)\nif \\(A\\) and \\(B\\) are the same size,\n\\(A + B\\)\n\\(B - B\\)\n\\(cA + rB\\) <- this is also called a linear combination of A and B\n\\[A = \\begin{bmatrix}\n1 & -1 & 1\\\\\n0 & 2 & 4\n\\end{bmatrix}\n\\]\n\nA = [[1,-1,1],[0,2,4]]\nB = [[2,-10,7], [3,8,6]]\n\nA+2*B # okay, this actually doesn't work\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\n\nmatrix1 <- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)\nmatrix2 <- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)\n\nmatrix1*matrix2 # OKAY THIS DOESNT WORK EITHER\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\n\nMatrix multiplication\ndot product result is ALWAYS A NUMBER - use dot product to create matrix multiplication\nNotation:\n\nQuantitative output: Y\nQualitative output: G\nMatrix: A\n\nLinear Models: - you would use regression for numerical - you would use classification for\n\\[\ny = mx+b\n\\]\nwhere x = input y = output w = \\(\\begin{bmatrix} m \\\\ b \\end{bmatrix}\\)\ngoal is to make the line of best fit of the weights\nGiven input vector \\(X^T = (X_1, X_2, ..., X_p)\\) we predict \\(Y\\) by\n\\[\n\\hat{Y}= \\hat{B} + (X_1\\hat{B_1}+ X_1\\hat{B_2} + ... + X_p\\hat{B_p})\n\\]\n\\[\n\\hat{Y}= \\sum_{j=1}^PX_j\\hat{B_j}\n\\]\nwhere $ $ is the intercept.\nIf you have a vector \\(X=\\begin{bmatrix}X_1 \\\\ X_2 \\\\ ... \\\\ X_p\\end{bmatrix}\\)\nlinear model is\n\\[\nf(x) = X^T\\hat{B}\n\\]\nbut many different techniques to have flavors of linear modesl\n\\[\nError = y - \\hat{Y}\n\\]\nHow to tell if your model is doing well:\nResidual sum square of errors and least square we pick coefficients (weights), \\(\\hat{B}\\) to minimize the RSS\n\\[\nRSS(\\hat{B})= \\sum_{j=1}^N(y_j-x_j^T\\hat{B})^2\n\\]\nSimple linear regression:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown\n\\[\n\\hat{y}=\\hat{B_0} + \\hat{B_1}x\n\\]\n\\[\n\\hat{y_i}=\\hat{B_0} + \\hat{B_1}x_i\n\\]\n(is the prediction of Y based on x_i)\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nwhere \\(y_i\\) is actual and \\(\\hat{y_i}\\) is predicted\n\n\nThe least sqaure is\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]"
  },
  {
    "objectID": "posts/CAP5765-week-2/wk2-cda-linear.html",
    "href": "posts/CAP5765-week-2/wk2-cda-linear.html",
    "title": "wk 2: linear regression",
    "section": "",
    "text": "Linear regression is a fabulous place to start with statistical learning. The math is (relatively) simple, but it can feel like thereâ€™s a lot of moving parts if youâ€™re new to all this. In general, we want to know 3 things this week about linear regression: create the best line possible, assess the coefficients and know if we did a good job.\n\n\n\nThe formula for least squares is:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\nSo we want to calculate out two things: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope).\nHere are the formulas:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThis course involves a lot of math, and variables. Itâ€™s hard to keep them straight if youâ€™re not used to looking at a lot of statistics.\nBars (\\(\\bar{x}\\)) are for means. Think of ðŸ˜ being a mean face.\nHats (\\(\\hat{x}\\)) are for estimated numbers. Think of \\(\\hat{y}_0\\) as wearing a hat to cosplay as their best guess of what \\(y_0\\) looks like ðŸ¤  (sometimes y is not very good at cosplaying).\n\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so letâ€™s not be too concerned with this right now.\n\n\n\nRounding up a few key points:\n\nThe difference between \\(\\hat{y}_i\\) and \\(y_i\\) is \\(e_1\\). This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nStandard error has a reverse relation with number of training set.\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n\n\nResidual Standard Error = \\(\\sqrt{-\\frac{RSS}{n-2}}\\)\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular olâ€™ residuals. We also need to divide by something to account for the degrees of freedom, thus \\(n-2\\). Interested in why we divide by 2? (tldr; residual degrees of freedom.)\nCONFIDENCE INTERVAL \\(\\hat{B_1} \\pm 2SE(\\hat{B_1})\\)\nWhat is a confidence interval tho? values that with 95% probability, the range will contain the true unknown value\nHereâ€™s a thread to help understand:\n\n\nHey Stats folk, what's your 280 character definition of a confidence interval?ðŸ¤”\n\nâ€” Chelsea Parlett-Pelleriti (@ChelseaParlett) March 13, 2018\n\n\n\n\n\n\n\n\n\\(H_0:\\) There is no relation between X and Y, that is \\(H_0:\\beta_1=0\\) \\(H_A:\\) There is some relationship between X and Y, that is \\(H_A:\\beta_1\\ne0\\)\nif \\(\\beta_1 = 0\\),\nto test null, we do a \\(t\\) statistic:\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nwhat is a p-value?\n\n\na small p-value is goodâ€“ it means our model is accurate\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\nhaving a large p-valueâ€“ less likely there is a relation between X and Y\n\n\n\nRSE is used to either estimate the STANDARD ERROR of \\(\\beta_x\\) OR the accuracy of the overall model\n$ R^2 = = 1 - $ for how much variance is explained. A sanity check here: R^2 should always be [0,1]. As a modelâ€™s RSS shrinks, R^2 will get closer and closer to 1.\nWe already know RSS and \\(TSS = \\sum_{i=1}^n(y_i-\\bar{y})\\)\n\n\n\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cap5765 notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "iz",
    "section": "",
    "text": "cheers,\nisabel zimmerman"
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html",
    "href": "posts/week-2/wk2-cda-linear.html",
    "title": "wk 2: linear regression",
    "section": "",
    "text": "Linear regression is a fabulous place to start with statistical learning. The math is (relatively) simple, but it can feel like thereâ€™s a lot of moving parts if youâ€™re new to all this. In general, we want to know 3 things this week about linear regression: create the best line possible, assess the coefficients and know if we did a good job.\n\n\n\nThe formula for least squares is:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\nSo we want to calculate out two things: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope).\nHere are the formulas:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThis course involves a lot of math, and variables. Itâ€™s hard to keep them straight if youâ€™re not used to looking at a lot of statistics.\nBars (\\(\\bar{x}\\)) are for means. Think of ðŸ˜ being a mean face.\nHats (\\(\\hat{x}\\)) are for estimated numbers. Think of \\(\\hat{y}_0\\) as wearing a hat to cosplay as their best guess of what \\(y_0\\) looks like ðŸ¤  (sometimes y is not very good at cosplaying).\n\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so letâ€™s not be too concerned with this right now.\n\n\n\nRounding up a few key points:\n\nThe difference between \\(\\hat{y}_i\\) and \\(y_i\\) is \\(e_1\\). This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nStandard error has a reverse relation with number of training set.\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n\n\nResidual Standard Error = \\(\\sqrt{-\\frac{RSS}{n-2}}\\)\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular olâ€™ residuals. We also need to divide by something to account for the degrees of freedom, thus \\(n-2\\). Interested in why we divide by 2? (tldr; residual degrees of freedom.)\nCONFIDENCE INTERVAL \\(\\hat{B_1} \\pm 2SE(\\hat{B_1})\\)\nWhat is a confidence interval tho? values that with 95% probability, the range will contain the true unknown value\nHereâ€™s a thread to help understand:\n\n\nHey Stats folk, what's your 280 character definition of a confidence interval?ðŸ¤”\n\nâ€” Chelsea Parlett-Pelleriti (@ChelseaParlett) March 13, 2018\n\n\n\n\n\n\n\n\nModels are toddlers. They LOVE saying NO. You might say, â€œThese coefficents mean something!â€ â€œNOâ€. While you can win toddlers (and college students) over with mac and cheese, models need cold, hard facts.\n\\(H_0:\\) There is no relation between X and Y, that is \\(H_0:\\beta_1=0\\) \\(H_A:\\) There is some relationship between X and Y, that is \\(H_A:\\beta_1\\ne0\\)\nif \\(\\beta_1 = 0\\),\nto test null, we do a \\(t\\) statistic:\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nwhat is a p-value?\n\n\na small p-value is goodâ€“ it means our model is accurate\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\nhaving a large p-valueâ€“ less likely there is a relation between X and Y\n\n\n\n\nRSE\n\\(R^2\\)\nF-statistic\n\nRSE is used to either estimate the STANDARD ERROR of \\(\\beta_x\\) OR the accuracy of the overall model\nWe already know RSS and \\(TSS = \\sum_{i=1}^n(y_i-\\bar{y})\\).\n$ R^2 = = 1 - $ for how much variance in Y is explained by X. As a modelâ€™s RSS shrinks, \\(R^2\\) will get closer and closer to 1.\n\n\n\n\n\n\nTip\n\n\n\nA sanity check here: \\(R^2\\) should always be [0,1]. If your \\(R^2\\) is bigger than 1 or larger than 0â€¦something is going very wrong.\n\n\nIf:\n\\[\nTSS = \\sum_{i=1}^n((y-\\bar{y})+(\\hat{y}_i-\\bar{y}))^2\n\\]\nwhy is?\n\\[\n\\sum_{i=1}^n(y_i-\\hat{y_i})(\\hat{y}_i-\\bar{y_i}) = 0\n\\]\nF statistic: If you get a large f value (one that is bigger than the F critical value found in a table), it means something is significant. In general, if F > 1, you will reject the null hypothesis. If F<1, coefficient is nonzero.\n\n\n\nAre at least one predictor \\(X_1, X_2, ... X_p\\) useful in predicting? (ie, p-value<0.05 and nonzero. Okay, so the coefficient is how much an input changes. If something has no effect, the coefficient will be zero (or very close to it))\nDo all the predictors help to explain Y, or just a few? (reminder, thereâ€™s \\(2^P\\) subsetsâ€¦ weâ€™re not going to check all of them.)\nHow well does the model fit?\nGiven a\n\n\n\nInterpret \\(\\beta_j\\) as the average effect on Y as a one unit increase. Hold all other variables constant.\n\n\n\nwhat to do with insignificant p-values?\n\n\n\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "posts/week-2/wk2-deeper-into-reg.html",
    "href": "posts/week-2/wk2-deeper-into-reg.html",
    "title": "wk 2: deeper into linear regression",
    "section": "",
    "text": "We understand linear regression, but we left with the FEAR OF EXPONENTIAL GROWTH given to usâ€¦as we get into mutliple regression, we have to compare \\(2^P\\) (P being the number of variables) models.\n\nhttps://counting.substack.com/p/what-if-you-were-an-evil-data-scientist\n\n\nComputer least squares fit for all possible subsets then choose between them. If we have 40 variables, this is computing BILLIONS OF MODELS. This is really expensive ()\n\n\n\n\n\n\nTip\n\n\n\nAlgorithms can be GREEDY.\n\n\nstart with Null subset, ie, \\(y = \\beta_0\\)\nstep 2 For \\(K = 1, ... , p\\) fit all (p k) models that contain exactly k predictors\nK = 1: \\(M_1: t = \\beta_1 + \\beta_1X_1\\) \\(M_2: t = \\beta_2 + \\beta_2X_2\\) â€¦ \\(M_{10}: t = \\beta_{10} + \\beta_{10}X_{10}\\)\nstep 3 select single best method among M_0\n\n\n\nBefore the world of American Ninja Warrior, there was Wipeout!\nForwards selection\nBackwards starts with all variables, remove variable with largest p-value\n\n\n\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "posts/week-1/wk1-cda-intro.html",
    "href": "posts/week-1/wk1-cda-intro.html",
    "title": "wk 1: vocab and linear algebra",
    "section": "",
    "text": "Learning primer:\nOn types of data:\n- Unstructured data does not follow any rhyme or reason\n- Semi-structured has some consitency, but no precise structure\n- Structured data has persistent order\nOn model outputs:\n- Quantitative data can be measured (numeric)\n- Qualitative data is descriptive (categorical) but qualitatative\nOn types of model:\n- Supervised learning is labeled data\n    - Some issues in supervised learning could be overfitting\n- Unsupervised learning is unlabeled data\n    - Some issues could be know\nmatricies: rectangular array, size (row, column) special <3 for SQUARE (n,n) matricies\n\\[\nA=[a_{ij}]\n\\]\n(the entry on row i and column j of matrix A)\nif \\(A\\) and \\(B\\) are the same size,\n\\(A + B\\)\n\\(B - B\\)\n\\(cA + rB\\) <- this is also called a linear combination of A and B\n\\[A = \\begin{bmatrix}\n1 & -1 & 1\\\\\n0 & 2 & 4\n\\end{bmatrix}\n\\]\nIf youâ€™re like me and immediately think â€œoh yeah, Iâ€™m going to mae a computer do that,â€ be forewarnedâ€“\nPython:\n\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\nR:\n\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\nSo neither of these languages support matrix multiplication the way you multiply numbers. How do we do this?\nPython:\n\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\nR:\n\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\n\nMatrix multiplication\ndot product result is ALWAYS A NUMBER - use dot product to create matrix multiplication\nNotation:\n\nQuantitative output: Y\nQualitative output: G\nMatrix: A\n\nLinear Models: - you would use regression for numerical - you would use classification for\n\\[\ny = mx+b\n\\]\nwhere x = input y = output w = \\(\\begin{bmatrix} m \\\\ b \\end{bmatrix}\\)\ngoal is to make the line of best fit of the weights\nGiven input vector \\(X^T = (X_1, X_2, ..., X_p)\\) we predict \\(Y\\) by\n\\[\n\\hat{Y}= \\hat{B} + (X_1\\hat{B_1}+ X_1\\hat{B_2} + ... + X_p\\hat{B_p})\n\\]\n\\[\n\\hat{Y}= \\sum_{j=1}^PX_j\\hat{B_j}\n\\]\nwhere $ $ is the intercept.\nIf you have a vector \\(X=\\begin{bmatrix}X_1 \\\\ X_2 \\\\ ... \\\\ X_p\\end{bmatrix}\\)\nlinear model is\n\\[\nf(x) = X^T\\hat{B}\n\\]\nbut many different techniques to have flavors of linear modesl\n\\[\nError = y - \\hat{Y}\n\\]\nHow to tell if your model is doing well:\nResidual sum square of errors and least square we pick coefficients (weights), \\(\\hat{B}\\) to minimize the RSS\n\\[\nRSS(\\hat{B})= \\sum_{j=1}^N(y_j-x_j^T\\hat{B})^2\n\\]\nSimple linear regression:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown\n\\[\n\\hat{y}=\\hat{B_0} + \\hat{B_1}x\n\\]\n\\[\n\\hat{y_i}=\\hat{B_0} + \\hat{B_1}x_i\n\\]\n(is the prediction of Y based on x_i)\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nwhere \\(y_i\\) is actual and \\(\\hat{y_i}\\) is predicted\n\n\nThe least sqaure is\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]"
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#create-the-line",
    "href": "posts/week-2/wk2-cda-linear.html#create-the-line",
    "title": "wk 2: linear regression",
    "section": "Create the line",
    "text": "Create the line\nThe formula for least squares is:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\nSo we want to calculate out two things: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope).\nHere are the formulas:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nTip\n\n\nThis course involves a lot of math, and variables. Itâ€™s hard to keep them straight if youâ€™re not used to looking at a lot of statistics.\nBars (\\(\\bar{x}\\)) are for means. Think of ðŸ˜ being a mean face.\nHats (\\(\\hat{x}\\)) are for estimated numbers. Think of \\(\\hat{y}_0\\) as wearing a hat to cosplay as their best guess of what \\(y_0\\) looks like ðŸ¤  (sometimes y is not very good at cosplaying).\n\n\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so letâ€™s not be too concerned with this right now."
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#assess-coefficient-estimates",
    "href": "posts/week-2/wk2-cda-linear.html#assess-coefficient-estimates",
    "title": "wk 2: linear regression",
    "section": "Assess coefficient estimates",
    "text": "Assess coefficient estimates\nRounding up a few key points:\n\nThe difference between \\(\\hat{y}_i\\) and \\(y_i\\) is \\(e_1\\). This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\n\n\n\n\n\nTip\n\n\nStandard error has a reverse relation with number of training set.\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n\n\n\nResidual Standard Error = \\(\\sqrt{-\\frac{RSS}{n-2}}\\)\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular olâ€™ residuals. We also need to divide by something to account for the degrees of freedom, thus \\(n-2\\). Interested in why we divide by 2? (tldr; residual degrees of freedom.)\nCONFIDENCE INTERVAL \\(\\hat{B_1} \\pm 2SE(\\hat{B_1})\\)\nWhat is a confidence interval tho? values that with 95% probability, the range will contain the true unknown value\nHereâ€™s a thread to help understand:\n\n\nHey Stats folk, what's your 280 character definition of a confidence interval?ðŸ¤”\n\nâ€” Chelsea Parlett-Pelleriti (@ChelseaParlett) March 13, 2018\n\n\n\nkey takeaways:"
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#hypothesis-testing",
    "href": "posts/week-2/wk2-cda-linear.html#hypothesis-testing",
    "title": "wk 2: linear regression",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nModels are toddlers. They LOVE saying NO. You might say, â€œThese coefficents mean something!â€ â€œNOâ€. While you can win toddlers (and college students) over with mac and cheese, models need cold, hard facts.\n\\(H_0:\\) There is no relation between X and Y, that is \\(H_0:\\beta_1=0\\) \\(H_A:\\) There is some relationship between X and Y, that is \\(H_A:\\beta_1\\ne0\\)\nif \\(\\beta_1 = 0\\),\nto test null, we do a \\(t\\) statistic:\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\n\n\n\n\n\nTip\n\n\nwhat is a p-value?\n\n\n\na small p-value is goodâ€“ it means our model is accurate\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\nhaving a large p-valueâ€“ less likely there is a relation between X and Y"
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#assessing-overall-accuracy-of-the-model",
    "href": "posts/week-2/wk2-cda-linear.html#assessing-overall-accuracy-of-the-model",
    "title": "wk 2: linear regression",
    "section": "Assessing overall accuracy of the model",
    "text": "Assessing overall accuracy of the model\n\nRSE\n\\(R^2\\)\nF-statistic\n\nRSE is used to either estimate the STANDARD ERROR of \\(\\beta_x\\) OR the accuracy of the overall model\nWe already know RSS and \\(TSS = \\sum_{i=1}^n(y_i-\\bar{y})\\).\n$ R^2 = = 1 - $ for how much variance in Y is explained by X. As a modelâ€™s RSS shrinks, \\(R^2\\) will get closer and closer to 1.\n\n\n\n\n\n\nTip\n\n\nA sanity check here: \\(R^2\\) should always be [0,1]. If your \\(R^2\\) is bigger than 1 or larger than 0â€¦something is going very wrong.\n\n\n\nIf:\n\\[\nTSS = \\sum_{i=1}^n((y-\\bar{y})+(\\hat{y}_i-\\bar{y}))^2\n\\]\nwhy is?\n\\[\n\\sum_{i=1}^n(y_i-\\hat{y_i})(\\hat{y}_i-\\bar{y_i}) = 0\n\\]\nF statistic: If you get a large f value (one that is bigger than the F critical value found in a table), it means something is significant. In general, if F > 1, you will reject the null hypothesis. If F<1, coefficient is nonzero."
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#assessing-model-quality",
    "href": "posts/week-2/wk2-cda-linear.html#assessing-model-quality",
    "title": "wk 2: linear regression",
    "section": "Assessing model quality",
    "text": "Assessing model quality\nAre at least one predictor \\(X_1, X_2, ... X_p\\) useful in predicting? (ie, p-value<0.05 and nonzero. Okay, so the coefficient is how much an input changes. If something has no effect, the coefficient will be zero (or very close to it))\nDo all the predictors help to explain Y, or just a few? (reminder, thereâ€™s \\(2^P\\) subsetsâ€¦ weâ€™re not going to check all of them.)\nHow well does the model fit?\nGiven a"
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#multiple-linear-regression",
    "href": "posts/week-2/wk2-cda-linear.html#multiple-linear-regression",
    "title": "wk 2: linear regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nInterpret \\(\\beta_j\\) as the average effect on Y as a one unit increase. Hold all other variables constant.\n\n\n\nwhat to do with insignificant p-values?"
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html#important-functions",
    "href": "posts/week-2/wk2-cda-linear.html#important-functions",
    "title": "wk 2: linear regression",
    "section": "Important functions",
    "text": "Important functions\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "posts/week-3/wk3-deeper-into-reg.html",
    "href": "posts/week-3/wk3-deeper-into-reg.html",
    "title": "wk 3: deeper into regression",
    "section": "",
    "text": "We understand linear regression, but we left with the FEAR OF EXPONENTIAL GROWTH given to usâ€¦as we get into mutliple regression, we have to compare \\(2^P\\) (P being the number of variables) models.\n\n\nComputer least squares fit for all possible subsets then choose between them. If we have 40 variables, this is computing BILLIONS OF MODELS. This is really expensive and it makes things unnecessarily complex.\n\n\n\n\n\n\nTip\n\n\n\nAlgorithms can be GREEDY. Greedy algortihms pick the best immediate output, but does not consider the big picture.\n\n\nFor forward selection:\nstep 1 start with null subset, ie, \\(y = \\beta_0\\)\nstep 2 For \\(K = 1, ... , p\\) fit all (p k) models that contain exactly k predictors\nK = 1:\n\\(M_1: t = \\beta_1 + \\beta_1X_1\\) \\(M_2: t = \\beta_2 + \\beta_2X_2\\) â€¦ \\(M_{10}: t = \\beta_{10} + \\beta_{10}X_{10}\\)\nstep 3 select single best method among M_0\n\n\n\nBackwards starts with all variables, remove variable with largest p-value\nBest subset https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856\n(what about racing?)\nwhen we are using stepwise, we want to minimize training set MSE. sure, but what about the MSE for the test set?\n\n\n\n\n\n\nNote\n\n\n\nThe MSE for a test set is quadratic, where the MSE for a training set is negative linear.\n\n\nTraining MSE is generally an underestimate of the test MSE, where \\(MSE = RSS/n\\) In fitting a model to the training data using least squares: - regression coefficients are estimated so that RSS is as small as possible - training sets for RSS and \\(R^2\\) cannot be used\n\n\n\nIRL, stepwise is not the best to use.\nPulling out the highlights from the link:\n\n\nIt yields R-squared values that are badly biased to be high.\nThe F and chi-squared tests quoted next to each variable on the printout do not have the claimed distribution.\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]).\nIt has severe problems in the presence of collinearity.\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\nIt allows us to not think about the problem.\nIt uses a lot of paper.\n\n\n\n\n\nBig things to remember here: \\(C_p\\), \\(AIC\\), \\(BIC\\), and \\(R^2\\).\n\n\\(C_p\\)Akaike information criterion (AIC)Bayesian information criterion (BIC)Adjusted \\(R^2\\)\n\n\n\\(C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\\)\n\nd is number of predictors\n\\(\\sigma\\) is estimate of the variance of error for each response\n\\(2d\\hat{\\sigma}^2\\) is the penalty. penalty increase as number of predictors increase\n\nif \\(\\hat{\\sigma}^2\\) is unbiased estimate, then \\(C_p\\) is an unbiased estimate of test MSE\nwhat does bias mean? data bias? model bias? anything that is not a normal distribution?\nSmaller is better.\n\n\n\\(AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2)\\) which simplifies to \\(AIC = \\frac{C_p}{\\hat{\\sigma}^2}\\)\n\ndirectly proportional to \\(C_p\\)\n\nSmaller is better.\n\n\n\\(BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\\)\npenalty is \\(log(n)d\\hat{\\sigma}^2)\\)\nbecuase it is using log, BIC penalizes large models more than AIC, which always uses the penalty of 2 (this is bc log(7))\nSmaller is better.\n\n\n$ R^2 = 1 - \n\nit can be negative, but that means thiings are going very bad. it cannot be greater than 1 (you canâ€™t explain more than 100% of your model). negative values occur when your model is worse than just always guessing the average of all the data points More info on negative \\(R^2\\)\nMAXIMIZE!\n\n\n\n\n\n\nKinda like subset, but better. You try to make some of the coefficients to zero, aka shrinking them. Use all the predictors. Ridge regression and lasso are the most famous. (Donâ€™t shrink the intercept, only the predictors.)\n\\[\ny = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\n\\]\nQ: can using a shrinkage method increase variance?\n\n\nFormula:\n\\[\n\\sum_i(y_i-\\beta_0-\\sum_j\\beta_jX_{ij})^2\n\\]\nRidge is good in terms of bias-variance trade off. As \\(\\lambda\\) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n\\(\\lambda\\) if lambda is 0, the penalty is the same as the RSS. when lambda is greater than 0, RSS is greater.\nminimize this quanity, so each lambda\n\n\\(\\lambda\\sum\\beta_j^2\\) is shrinkage penalty\n\\(\\lambda\\) is tuning parameter\n\\(\\lambda \\ge 0\\)\n\nwhen \\(\\lambda = 0\\), it is just RSS\nThe coefficient estimate obtained by Ridge Regression are denoted \\(\\beta_\\lambda^R\\) for each \\(\\lambda\\)\nall these \\(\\beta_j^R\\) depend on \\(\\lambda\\) (\\(\\beta_j^R\\) <- is a vector when p > 1)\nselecting good value of \\(\\lambda\\) is critical (we will use cross validation) ridge uses the \\(l_2\\) norm is $||\\_2 = ||^2 $\na range of 0/least squares to least squares/least squares (1)\ncon: ridge retains all coefficients, no selection\n\n\n\non a scale of least squares to zero, how are you feeling today?\n\\[\n\\sum_i(y_i-\\beta_0-\\sum_j\\beta_jX_{ij})^2 +\\lambda\\sum_j|\\beta_j|\n\\]\nfunction of data, but also function of \\(\\lambda\\)\nfix \\(\\lambda\\) and then do minimization problem. if \\(\\lambda\\) is near 0, it esentially is just least squared. if the \\(\\lambda\\) approaches infinity, ridge coefficient estimates shrink to zero\nlasso uses \\(l_1\\) norm is $ ||\\_1 = ||$\nlasso does not retain all of the coefficients, so it is in fact a selection method\nOne obvious advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors."
  }
]