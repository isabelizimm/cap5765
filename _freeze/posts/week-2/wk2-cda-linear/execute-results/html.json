{
  "hash": "5556884377700a8d4396bf4f3c9da9f4",
  "result": {
    "markdown": "---\nformat: revealjs\ntitle: \"wk 2: linear regression\"\n---\n\n\n# Linear regression\n\nLinear regression is a fabulous place to start with statistical learning. The math is (relatively) simple, but it can feel like there's a lot of moving parts if you're new to all this. In general, we want to know 3 things this week about linear regression: [create the best line possible](#create-the-line), [assess the coefficients](#assess-coefficient-estimates) and [know if we did a good job](#assessing-overall-accuracy-of-the-model).\n\n![](https://miro.medium.com/max/1400/1*N1-K-A43_98pYZ27fnupDA.jpeg)\n\n## Create the line\n\nThe formula for least squares is:\n\n\n$$\nY \\approx \\beta_0 + \\beta_1X\n$$\n\n\nSo we want to calculate out two things: $\\beta_0$ (intercept) and $\\beta_1$ (slope).\n\nHere are the formulas:\n\n$$\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n$$\n\n$$\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n$$\n\n\n:::{.callout-tip}\nThis course involves a lot of math, and variables. It's hard to keep them straight if you're not used to looking at a lot of statistics.\n\nBars ($\\bar{x}$) are for means. Think of üòê being a mean face.\n\nHats ($\\hat{x}$) are for estimated numbers. Think of $\\hat{y}_0$ as wearing a hat to cosplay as their best guess of what $y_0$ looks like ü§† (sometimes y is not very good at cosplaying).\n:::\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so let's not be too concerned with this right now.\n\n## Assess coefficient estimates\n\nRounding up a few key points:\n\n - The difference between $\\hat{y}_i$ and $y_i$ is $e_1$. This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\n\n$$\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n$$\n\n$$\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n$$\n\n\n:::{.callout-tip}\nStandard error has a reverse relation with number of training set.\n\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n:::\n\nResidual Standard Error = $\\sqrt{-\\frac{RSS}{n-2}}$ \n\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular ol' residuals. We also need to divide by *something* to account for the degrees of freedom, thus $n-2$. [Interested in why we divide by 2?](https://stats.stackexchange.com/questions/204238/why-divide-rss-by-n-2-to-get-rse) (tldr; residual degrees of freedom.)\n\nCONFIDENCE INTERVAL $\\hat{B_1} \\pm 2SE(\\hat{B_1})$\n\nWhat is a confidence interval tho?\n_values that with 95% probability, the range will contain the true unknown value_\n\nHere's a thread to help understand:\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Hey Stats folk, what&#39;s your 280 character definition of a confidence interval?ü§î</p>&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) <a href=\"https://twitter.com/ChelseaParlett/status/973657698536366080?ref_src=twsrc%5Etfw\">March 13, 2018</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\n- key takeaways:\n  - \n\n## Hypothesis testing\n\nModels are toddlers. They LOVE saying NO. You might say, \"These coefficents mean something!\" \"NO\". While you can win toddlers (and college students) over with mac and cheese, models need cold, hard facts.\n\n$H_0:$ There is no relation between X and Y, that is $H_0:\\beta_1=0$\n$H_A:$ There is some relationship between X and Y, that is $H_A:\\beta_1\\ne0$ \n\nif $\\beta_1 = 0$, \n\nto test null, we do a $t$ statistic: \n\n\n$$\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n$$\n\n\n:::{.callout-tip}\nwhat is a p-value?\n:::\n\na small p-value is good-- it means our model is accurate\n\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\n\nhaving a large p-value-- less likely there is a relation between X and Y\n\n## Assessing overall accuracy of the model\n\n- RSE\n- $R^2$\n- F-statistic\n\nRSE is used to either estimate the STANDARD ERROR of $\\beta_x$ OR the accuracy of the overall model\n\nWe already know RSS and $TSS = \\sum_{i=1}^n(y_i-\\bar{y})$.\n\n$ R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} $ for how much variance in Y is explained by X. As a model's RSS shrinks, $R^2$ will get closer and closer to 1.\n\n:::{.callout-tip}\nA sanity check here: $R^2$ should always be [0,1]. If your $R^2$ is bigger than 1 or larger than 0...something is going very wrong.\n:::\n\nIf:\n\n$$\nTSS = \\sum_{i=1}^n((y-\\bar{y})+(\\hat{y}_i-\\bar{y}))^2\n$$\n\n\nwhy is?\n\n$$\n\\sum_{i=1}^n(y_i-\\hat{y_i})(\\hat{y}_i-\\bar{y_i}) = 0\n$$\n\n\n\nF statistic: If you get a large f value (one that is bigger than the F critical value found in a table), it means something is significant. In general, if F > 1, you will reject the null hypothesis. If F<1, coefficient is nonzero.\n\n## Assessing model quality\n\nAre at least one predictor $X_1, X_2, ... X_p$ useful in predicting? (ie, p-value<0.05 and nonzero. Okay, so the coefficient is how much an input changes. If something has no effect, the coefficient will be zero (or very close to it))\n\nDo all the predictors help to explain Y, or just a few? (reminder, there's $2^P$ subsets... we're not going to check all of them.)\n\nHow well does the model fit?\n\nGiven a \n\n## Multiple Linear Regression\n\nInterpret $\\beta_j$ as the average effect on Y as a one unit increase. Hold all other variables constant.\n\n\n::: {.cell}\n\n:::\n\n\n\nwhat to do with insignificant p-values?\n\n\n\n## Important functions\n\n\n$$\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n$$\n\n$$\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n$$\n\n$$\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n$$\n\n$$\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n$$\n\n$$\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n$$",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}