[
  {
    "objectID": "posts/models-overview/models.html",
    "href": "posts/models-overview/models.html",
    "title": "cap5765 notes",
    "section": "",
    "text": "will collect and highlight"
  },
  {
    "objectID": "posts/week-4/week-4.html",
    "href": "posts/week-4/week-4.html",
    "title": "wk 4 recap",
    "section": "",
    "text": "to be updated w notes (how to do tidymodels)"
  },
  {
    "objectID": "posts/week-3/wk3-deeper-into-reg.html",
    "href": "posts/week-3/wk3-deeper-into-reg.html",
    "title": "wk 3: deeper into regression",
    "section": "",
    "text": "We understand linear regression, but we left with the FEAR OF EXPONENTIAL GROWTH given to us‚Ä¶as we get into mutliple regression, we have to compare \\(2^P\\) (P being the number of variables) models.\n\n\nComputer least squares fit for all possible subsets then choose between them. If we have 40 variables, this is computing BILLIONS OF MODELS. This is really expensive and it makes things unnecessarily complex.\n\n\n\n\n\n\nTip\n\n\n\nAlgorithms can be GREEDY. Greedy algortihms pick the best immediate output, but does not consider the big picture.\n\n\nFor forward selection:\nstep 1 start with null subset, ie, \\(y = \\beta_0\\)\nstep 2 For \\(K = 1, ... , p\\) fit all (p k) models that contain exactly k predictors\nK = 1:\n\\(M_1: t = \\beta_1 + \\beta_1X_1\\) \\(M_2: t = \\beta_2 + \\beta_2X_2\\) ‚Ä¶ \\(M_{10}: t = \\beta_{10} + \\beta_{10}X_{10}\\)\nstep 3 select single best method among M_0\n\n\n\nBackwards starts with all variables, remove variable with largest p-value\nBest subset https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856\n(what about racing?)\nwhen we are using stepwise, we want to minimize training set MSE. sure, but what about the MSE for the test set?\n\n\n\n\n\n\nNote\n\n\n\nThe MSE for a test set is quadratic, where the MSE for a training set is negative linear.\n\n\nTraining MSE is generally an underestimate of the test MSE, where \\(MSE = RSS/n\\) In fitting a model to the training data using least squares: - regression coefficients are estimated so that RSS is as small as possible - training sets for RSS and \\(R^2\\) cannot be used\n\n\n\nIRL, stepwise is not the best to use.\nPulling out the highlights from the link:\n\n\nIt yields R-squared values that are badly biased to be high.\nThe F and chi-squared tests quoted next to each variable on the printout do not have the claimed distribution.\nThe method yields confidence intervals for effects and predicted values that are falsely narrow; see Altman and Andersen (1989).\nIt yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]).\nIt has severe problems in the presence of collinearity.\nIt is based on methods (e.g., F tests for nested models) that were intended to be used to test prespecified hypotheses.\nIncreasing the sample size does not help very much; see Derksen and Keselman (1992).\nIt allows us to not think about the problem.\nIt uses a lot of paper.\n\n\n\n\n\nBig things to remember here: \\(C_p\\), \\(AIC\\), \\(BIC\\), and \\(R^2\\).\n\n\\(C_p\\)Akaike information criterion (AIC)Bayesian information criterion (BIC)Adjusted \\(R^2\\)\n\n\n\\(C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\\)\n\nd is number of predictors\n\\(\\sigma\\) is estimate of the variance of error for each response\n\\(2d\\hat{\\sigma}^2\\) is the penalty. penalty increase as number of predictors increase\n\nif \\(\\hat{\\sigma}^2\\) is unbiased estimate, then \\(C_p\\) is an unbiased estimate of test MSE\nwhat does bias mean? data bias? model bias? anything that is not a normal distribution?\nSmaller is better.\n\n\n\\(AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2)\\) which simplifies to \\(AIC = \\frac{C_p}{\\hat{\\sigma}^2}\\)\n\ndirectly proportional to \\(C_p\\)\n\nSmaller is better.\n\n\n\\(BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\\)\npenalty is \\(log(n)d\\hat{\\sigma}^2)\\)\nbecuase it is using log, BIC penalizes large models more than AIC, which always uses the penalty of 2 (this is bc log(7))\nSmaller is better.\n\n\n$ R^2 = 1 - \n\nit can be negative, but that means thiings are going very bad. it cannot be greater than 1 (you can‚Äôt explain more than 100% of your model). negative values occur when your model is worse than just always guessing the average of all the data points More info on negative \\(R^2\\)\nMAXIMIZE!\n\n\n\n\n\n\nKinda like subset, but better. You try to make some of the coefficients to zero, aka shrinking them. Use all the predictors. Ridge regression and lasso are the most famous. (Don‚Äôt shrink the intercept, only the predictors.)\n\\[\ny = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\n\\]\nQ: can using a shrinkage method increase variance?\n\n\nFormula: \\[\n\\sum_i(y_i-\\beta_0-\\sum_j\\beta_jX_{ij})^2\n\\]\nRidge is good in terms of bias-variance trade off. As \\(\\lambda\\) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n\\(\\lambda\\) if lambda is 0, the penalty is the same as the RSS. when lambda is greater than 0, RSS is greater.\nminimize this quanity, so each lambda\n\n\\(\\lambda\\sum\\beta_j^2\\) is shrinkage penalty\n\\(\\lambda\\) is tuning parameter\n\\(\\lambda \\ge 0\\)\n\nwhen \\(\\lambda = 0\\), it is just RSS\nThe coefficient estimate obtained by Ridge Regression are denoted \\(\\beta_\\lambda^R\\) for each \\(\\lambda\\)\nall these \\(\\beta_j^R\\) depend on \\(\\lambda\\) (\\(\\beta_j^R\\) <- is a vector when p > 1)\nselecting good value of \\(\\lambda\\) is critical (we will use cross validation) ridge uses the \\(l_2\\) norm is $||\\_2 = ||^2 $\na range of 0/least squares to least squares/least squares (1)\ncon: ridge retains all coefficients, no selection\n\n\n\non a scale of least squares to zero, how are you feeling today?\n\\[\n\\sum_i(y_i-\\beta_0-\\sum_j\\beta_jX_{ij})^2 +\\lambda\\sum_j|\\beta_j|\n\\]\nfunction of data, but also function of \\(\\lambda\\)\nfix \\(\\lambda\\) and then do minimization problem. if \\(\\lambda\\) is near 0, it esentially is just least squared. if the \\(\\lambda\\) approaches infinity, ridge coefficient estimates shrink to zero\nlasso uses \\(l_1\\) norm is $ ||\\_1 = ||$\nlasso does not retain all of the coefficients, so it is in fact a selection method\nOne obvious advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors."
  },
  {
    "objectID": "posts/week-2/wk2-cda-linear.html",
    "href": "posts/week-2/wk2-cda-linear.html",
    "title": "wk 2: linear regression",
    "section": "",
    "text": "Linear regression is a fabulous place to start with statistical learning. The math is (relatively) simple, but it can feel like there‚Äôs a lot of moving parts if you‚Äôre new to all this. In general, we want to know 3 things this week about linear regression: create the best line possible, assess the coefficients and know if we did a good job.\n\n\n\nThe formula for least squares is:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\nSo we want to calculate out two things: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope).\nHere are the formulas:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThis course involves a lot of math, and variables. It‚Äôs hard to keep them straight if you‚Äôre not used to looking at a lot of statistics.\nBars (\\(\\bar{x}\\)) are for means. Think of üòê being a mean face.\nHats (\\(\\hat{x}\\)) are for estimated numbers. Think of \\(\\hat{y}_0\\) as wearing a hat to cosplay as their best guess of what \\(y_0\\) looks like ü§† (sometimes y is not very good at cosplaying).\n\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so let‚Äôs not be too concerned with this right now.\n\n\n\nRounding up a few key points:\n\nThe difference between \\(\\hat{y}_i\\) and \\(y_i\\) is \\(e_1\\). This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nStandard error has a reverse relation with number of training set.\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n\n\nResidual Standard Error = \\(\\sqrt{-\\frac{RSS}{n-2}}\\)\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular ol‚Äô residuals. We also need to divide by something to account for the degrees of freedom, thus \\(n-2\\). Interested in why we divide by 2? (tldr; residual degrees of freedom.)\nCONFIDENCE INTERVAL \\(\\hat{B_1} \\pm 2SE(\\hat{B_1})\\)\nWhat is a confidence interval tho? values that with 95% probability, the range will contain the true unknown value\nHere‚Äôs a thread to help understand:\n\n\nHey Stats folk, what's your 280 character definition of a confidence interval?ü§î\n\n‚Äî Chelsea Parlett-Pelleriti (@ChelseaParlett) March 13, 2018\n\n\n\n\n\n\n\n\nModels are toddlers. They LOVE saying NO. You might say, ‚ÄúThese coefficents mean something!‚Äù ‚ÄúNO‚Äù. While you can win toddlers (and college students) over with mac and cheese, models need cold, hard facts.\n\\(H_0:\\) There is no relation between X and Y, that is \\(H_0:\\beta_1=0\\) \\(H_A:\\) There is some relationship between X and Y, that is \\(H_A:\\beta_1\\ne0\\)\nif \\(\\beta_1 = 0\\),\nto test null, we do a \\(t\\) statistic:\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nwhat is a p-value?\n\n\na small p-value is good‚Äì it means our model is accurate\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\nhaving a large p-value‚Äì less likely there is a relation between X and Y\n\n\n\n\nRSE\n\\(R^2\\)\nF-statistic\n\nRSE is used to either estimate the STANDARD ERROR of \\(\\beta_x\\) OR the accuracy of the overall model\nWe already know RSS and \\(TSS = \\sum_{i=1}^n(y_i-\\bar{y})\\).\n$ R^2 = = 1 - $ for how much variance in Y is explained by X. As a model‚Äôs RSS shrinks, \\(R^2\\) will get closer and closer to 1.\n\n\n\n\n\n\nTip\n\n\n\nA sanity check here: \\(R^2\\) should always be [0,1]. If your \\(R^2\\) is bigger than 1 or larger than 0‚Ä¶something is going very wrong.\n\n\nIf:\n\\[\nTSS = \\sum_{i=1}^n((y-\\bar{y})+(\\hat{y}_i-\\bar{y}))^2\n\\]\nwhy is?\n\\[\n\\sum_{i=1}^n(y_i-\\hat{y_i})(\\hat{y}_i-\\bar{y_i}) = 0\n\\]\nF statistic: If you get a large f value (one that is bigger than the F critical value found in a table), it means something is significant. In general, if F > 1, you will reject the null hypothesis. If F<1, coefficient is nonzero.\n\n\n\nAre at least one predictor \\(X_1, X_2, ... X_p\\) useful in predicting? (ie, p-value<0.05 and nonzero. Okay, so the coefficient is how much an input changes. If something has no effect, the coefficient will be zero (or very close to it))\nDo all the predictors help to explain Y, or just a few? (reminder, there‚Äôs \\(2^P\\) subsets‚Ä¶ we‚Äôre not going to check all of them.)\nHow well does the model fit?\nGiven a\n\n\n\nInterpret \\(\\beta_j\\) as the average effect on Y as a one unit increase. Hold all other variables constant.\n\n\n\nwhat to do with insignificant p-values?\n\n\n\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "posts/week-5/week-5.html",
    "href": "posts/week-5/week-5.html",
    "title": "wk 5 recap",
    "section": "",
    "text": "LDA + QDA"
  },
  {
    "objectID": "posts/week-9/lda-qda-knn.html",
    "href": "posts/week-9/lda-qda-knn.html",
    "title": "wk 9: knn + pca",
    "section": "",
    "text": "what is the black box model?\nyou can‚Äôt see the inner workings of a model when you are using it in code‚Ä¶even models that are really not considered to be black box (ie, linear regression) can act as black boxes in practice.\n\n\nPCA\nmain goals: identify pattern in data\ndetect correlation btwn variables\nstrong correlation-> tries to reduce the dimensionality\nfinding the maximum variance in high dimensionality data and project to a smaller subspace while retaining most of the info\nsummary of PCA:\n\nstandardize/normalize the data\nobtain the eigenvector eigenspaces from covariance matrix\nperform singular value decomposition (SVD)\nsort eigenvalues in decreasing order and choose k largest eigenvalues, where k is # of dimensions of new feature subspaces\ntransform the original data set X variables to obtain feature subspace Y\n\nto understand after: largest eigenvalues is largest variety, explain variance of data in the new features (this is kinda like \\(R^2\\)?)\n\n\n\n\n\n\nTip\n\n\n\nwhat is the difference btwn correlation and covariance? they are the same, but correlation is scaled\n\n\nreduce dimensions and create fewer ones allow for viz many dimensions in 2 dim, or preprocessing for models that cant take high dimensions\nends up being a normalized linear combination of features.\nfirst principal component of a set of features \\(X_1, X_2, ..., X_p\\) is the normalized linear combination of the features\n\\[\nZ_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + ... + \\phi_{p1}X_p\n\\]\nthat has the largest variance\nWe mean that normalized that \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\).\nwe refer to \\(\\phi_{11}, ..., \\phi_{p1}\\) as the loadings of the first principal component.\nAnd think of them as the loading vector \\(\\phi_1\\)\nAssuming we have a \\(n \\times p\\) data set \\(\\mathbf{X}\\)\nsince we are only interested in the variance we assume that the variables have been centered\n\\[\n\\underset{\\phi_{11}, ..., \\phi_{p1}}{\\text{maximize}} \\left\\{ \\dfrac{1}{n} \\sum^n_{i=j} \\left( \\sum^p_{j=1} \\phi_{j1}x_{ij} \\right)^2 \\right\\} \\quad \\text{subject to} \\quad \\sum_{j=1}^p  \\phi_{j1}^2 = 1\n\\]\n(m<p) still thinking about \\(R^2\\)‚Äì so would you be able to use this? if PCA is essentially trying to figure out how to capture variance and is represented by explained variance, and \\(R^2\\) is a measure of explained variance‚Ä¶\nPCR -> principal component analysis using regression\n$ y = _0 + _1Z_1 + _2Z_2$ is the linear regression using PCA components \\(Z_1\\) and \\(Z_2\\)\ntldr; create new variables that are linear combinations of original variables linear combinations are uncorrelated new variables are principal components\nhow to choose number of components?\ngeometric explanation of PCA: you usually center and scale\nthe more correlated the original data, the better this line (first component) will explain the actual values of the observed measurements‚Äìgoal is to minimize sum of residuals (distance from origin to projected point is called the ‚Äúscore‚Äù)\nfind the first latent variable == first principal component\nwould you use PCA and then stop? or would you usually use it as feature engineering and then continue on\nKnowing relation btwn maximizing variance and how this is related to minimize residual\n\nwhat is SVD\nsingular value decomposition use it on covariance matricies, guarantee you find the \\(Z_1\\) has maximum variance, since it has the largest eigenvalue second component \\(Z_2\\) will be orthogonal to \\(Z_1\\), so it will be second largest variance and NOT CORRELATED with \\(Z_1\\) also reducing the dimension\n\n\n\nOn covariance\ncorrelation matrix is between -1 and 1. covariance any number.\ncovariance is practical esp w mulitvariate issues.\nvariance: each value subtracted from mean of that variable, differences squared, divided by number of values in that variable\nso COvariance is calculated between different variables: values of both variables are multiplied by taking the difference from the mean and diving by number of values cov(x,x) = var(x)\n\npositive: positive relationship, variate in the same direction\nnegative: negative relationship between 2 variables, variate in opposite directions\nzero: no relation between data\nsize of a covariance value: not scaled, so just bc numbers are high doesn‚Äôt mean that the covariance is high\n\nA <- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), 3, 3, byrow = TRUE)\n\n\nev <- eigen(A)\nvalues <- ev$values\n\n\nev$vectors\n\n           [,1]       [,2]      [,3]\n[1,]  0.7453560  0.6666667 0.0000000\n[2,] -0.5962848  0.6666667 0.4472136\n[3,]  0.2981424 -0.3333333 0.8944272\n\n\neigenvalues and eigenvectors of covariance matrix eigenvalues find magnitude and spread of data points when cov = 0, eigenvalues directly equal to variance values eigenvectors show direction\nuse these in PCA and LDA :D\nquiz on PCA + covariance not coding"
  },
  {
    "objectID": "posts/week-7/week-7.html",
    "href": "posts/week-7/week-7.html",
    "title": "wk 7 recap: midterm study guide",
    "section": "",
    "text": "Apologies for this week‚Ä¶its a little messy! Scratch notes for midterm review"
  },
  {
    "objectID": "posts/week-7/week-7.html#relation-between-x-and-y-in-linear-regression-vs.-logistic-regression",
    "href": "posts/week-7/week-7.html#relation-between-x-and-y-in-linear-regression-vs.-logistic-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Relation between X and Y in linear regression vs.¬†logistic regression",
    "text": "Relation between X and Y in linear regression vs.¬†logistic regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-linear-regression",
    "href": "posts/week-7/week-7.html#explain-linear-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain linear regression",
    "text": "Explain linear regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#brief-explanation-of-linear-regression",
    "href": "posts/week-7/week-7.html#brief-explanation-of-linear-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Brief explanation of linear regression",
    "text": "Brief explanation of linear regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#explain",
    "href": "posts/week-7/week-7.html#explain",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain:",
    "text": "Explain:\n\nBest subset selection\nStepwise selection\n\nForwards: start w 0, add in predictors\nBackwards: start w all, subtract predictors\nWhich is better?\n\nBest subset\n\nWhy?\n\nFW/BW: stepwise will bias R^2 values, model will smallest training RSS will be best subset, since it will encompass all the options from forward and backward stepwise, as well as interactions between these variables"
  },
  {
    "objectID": "posts/week-7/week-7.html#difference-between-ridge-and-lasso",
    "href": "posts/week-7/week-7.html#difference-between-ridge-and-lasso",
    "title": "wk 7 recap: midterm study guide",
    "section": "Difference between ridge and LASSO",
    "text": "Difference between ridge and LASSO\nComputing penalty Ridge \\(B_j^2\\), makes a O when you draw contours for error and constraint functions LASSO \\(|B_j|\\), makes a square when you draw contours for error and constraint functions Larger tuning parameter LASSO can be selection Think it can ‚Äúlasso‚Äù a portion of the variables Ridge all the predictors will remain in final model"
  },
  {
    "objectID": "posts/week-7/week-7.html#tf-lasso-and-ridge-compared-to-least-squares-are-less-flexible-and-will-give-improved-prediction-accuracy-when-its-increase-in-bias-is-less-than-its-decrease-in-variance.",
    "href": "posts/week-7/week-7.html#tf-lasso-and-ridge-compared-to-least-squares-are-less-flexible-and-will-give-improved-prediction-accuracy-when-its-increase-in-bias-is-less-than-its-decrease-in-variance.",
    "title": "wk 7 recap: midterm study guide",
    "section": "T/F: Lasso and Ridge, compared to least squares, are less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance.",
    "text": "T/F: Lasso and Ridge, compared to least squares, are less flexible and will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nbias-variance trade-off. ls have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias. This consequently can generate more accurate predictions. In addition, lasso performs variable selection which makes it easier to interpret than other methods like ridge regression."
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-logistic-regression",
    "href": "posts/week-7/week-7.html#explain-logistic-regression",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain logistic regression:",
    "text": "Explain logistic regression:\nLog odds of linear regression Coefficients found by maximum likelihood"
  },
  {
    "objectID": "posts/week-7/week-7.html#whether-a-student-will-passfail-in-an-exam-based-on-hours-of-study-can-be-solved-with-logisticlinear-regression.",
    "href": "posts/week-7/week-7.html#whether-a-student-will-passfail-in-an-exam-based-on-hours-of-study-can-be-solved-with-logisticlinear-regression.",
    "title": "wk 7 recap: midterm study guide",
    "section": "Whether a student will pass/fail in an exam based on hours of study can be solved with LOGISTIC/LINEAR regression.",
    "text": "Whether a student will pass/fail in an exam based on hours of study can be solved with LOGISTIC/LINEAR regression.\nWhy? Output variable is a classification, not a regression"
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-lda",
    "href": "posts/week-7/week-7.html#explain-lda",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain LDA",
    "text": "Explain LDA\nDim reduction Modeling distribution of predictors Like PCA that it reduces dimensions, but focuses on SEPERABILITY among known categories Creates a new acis and projects the data on this axis that maximizes separation 2 criteria, considered simultaneously: maximize distance between means (if high dim, use centroid), minimize variation w/in cat"
  },
  {
    "objectID": "posts/week-7/week-7.html#lda-assumptions",
    "href": "posts/week-7/week-7.html#lda-assumptions",
    "title": "wk 7 recap: midterm study guide",
    "section": "LDA assumptions",
    "text": "LDA assumptions\nobservations within each class come from a normal distribution with a class specific mean and a common variance œÉ2, and plugging estimates for these parameters into the Bayes classifier Number of parameters in LDA For LDA, (p+1) parameters are needed to construct the discriminant function in (2). For a problem with K classes, we would only need (K-1) such discriminant functions by arbitrarily choosing one class to be the base class (subtracting the base class likelihood from all other classes). Hence, the total number of estimated parameters for LDA is (K-1)(p+1).\nWhen there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.\nIf the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression."
  },
  {
    "objectID": "posts/week-7/week-7.html#explain-bias-variance-tradeoff",
    "href": "posts/week-7/week-7.html#explain-bias-variance-tradeoff",
    "title": "wk 7 recap: midterm study guide",
    "section": "Explain bias-variance tradeoff",
    "text": "Explain bias-variance tradeoff\nInability for a machine learning method to capture true relationship is BIAS (if it cant ever match, it is largely BIASED) (swayed by every single data point, no reservations, no backbone, no pre-determined bias to keep it safe) simple models tend to have higher bias Difference in fits between data sets is called VARIANCE (hard to predict how it will do with future data sets if high variance) complex models tend to have higher variance\nIdeally low bias, low variability"
  },
  {
    "objectID": "posts/week-7/week-7.html#what-is-over-fitting",
    "href": "posts/week-7/week-7.html#what-is-over-fitting",
    "title": "wk 7 recap: midterm study guide",
    "section": "What is over-fitting?",
    "text": "What is over-fitting?\nTraining model to be too flexible (not enough bias) You can reduce this by using cross fold validation\nEXTRAS:\nPCA: Focuses on dimension reduction by most variation\n(find the sweet spot with regularization, boosting, bagging)"
  },
  {
    "objectID": "posts/week-6/week-6.html",
    "href": "posts/week-6/week-6.html",
    "title": "wk 6 recap",
    "section": "",
    "text": "hurricane ian! no classes"
  },
  {
    "objectID": "posts/week-1/wk1-cda-intro.html",
    "href": "posts/week-1/wk1-cda-intro.html",
    "title": "wk 1: vocab and linear algebra",
    "section": "",
    "text": "Learning primer:\nOn types of data:\n- Unstructured data does not follow any rhyme or reason\n- Semi-structured has some consitency, but no precise structure\n- Structured data has persistent order\nOn model outputs:\n- Quantitative data can be measured (numeric)\n- Qualitative data is descriptive (categorical) but qualitatative\nOn types of model:\n- Supervised learning is labeled data\n    - Some issues in supervised learning could be overfitting\n- Unsupervised learning is unlabeled data\n    - Some issues could be know\nmatricies: rectangular array, size (row, column) special <3 for SQUARE (n,n) matricies\n\\[\nA=[a_{ij}]\n\\]\n(the entry on row i and column j of matrix A)\nif \\(A\\) and \\(B\\) are the same size,\n\\(A + B\\)\n\\(B - B\\)\n\\(cA + rB\\) <- this is also called a linear combination of A and B\n\\[A = \\begin{bmatrix}\n1 & -1 & 1\\\\\n0 & 2 & 4\n\\end{bmatrix}\n\\]\nIf you‚Äôre like me and immediately think ‚Äúoh yeah, I‚Äôm going to mae a computer do that,‚Äù be forewarned‚Äì\nPython:\n\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\nR:\n\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\nSo neither of these languages support matrix multiplication the way you multiply numbers. How do we do this?\nPython:\n\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\nR:\n\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\n\nMatrix multiplication\ndot product result is ALWAYS A NUMBER - use dot product to create matrix multiplication\nNotation:\n\nQuantitative output: Y\nQualitative output: G\nMatrix: A\n\nLinear Models: - you would use regression for numerical - you would use classification for\n\\[\ny = mx+b\n\\]\nwhere x = input y = output w = \\(\\begin{bmatrix} m \\\\ b \\end{bmatrix}\\)\ngoal is to make the line of best fit of the weights\nGiven input vector \\(X^T = (X_1, X_2, ..., X_p)\\) we predict \\(Y\\) by\n\\[\n\\hat{Y}= \\hat{B} + (X_1\\hat{B_1}+ X_1\\hat{B_2} + ... + X_p\\hat{B_p})\n\\]\n\\[\n\\hat{Y}= \\sum_{j=1}^PX_j\\hat{B_j}\n\\]\nwhere $ $ is the intercept.\nIf you have a vector \\(X=\\begin{bmatrix}X_1 \\\\ X_2 \\\\ ... \\\\ X_p\\end{bmatrix}\\)\nlinear model is\n\\[\nf(x) = X^T\\hat{B}\n\\]\nbut many different techniques to have flavors of linear modesl\n\\[\nError = y - \\hat{Y}\n\\]\nHow to tell if your model is doing well:\nResidual sum square of errors and least square we pick coefficients (weights), \\(\\hat{B}\\) to minimize the RSS\n\\[\nRSS(\\hat{B})= \\sum_{j=1}^N(y_j-x_j^T\\hat{B})^2\n\\]\nSimple linear regression:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown\n\\[\n\\hat{y}=\\hat{B_0} + \\hat{B_1}x\n\\]\n\\[\n\\hat{y_i}=\\hat{B_0} + \\hat{B_1}x_i\n\\]\n(is the prediction of Y based on x_i)\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nwhere \\(y_i\\) is actual and \\(\\hat{y_i}\\) is predicted\n\n\nThe least sqaure is\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]"
  },
  {
    "objectID": "posts/week-8/week-8.html",
    "href": "posts/week-8/week-8.html",
    "title": "wk 8 recap",
    "section": "",
    "text": "then i was out :("
  },
  {
    "objectID": "posts/week-10/bootstrapping.html",
    "href": "posts/week-10/bootstrapping.html",
    "title": "wk 10: bootstrapping",
    "section": "",
    "text": "Bootstrap\nThe bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method\nbootstrapping was originally a method for assessing statistical accuracy of an estimator\n\nfirst, each bootstrap sample must be of the same size as the original sample second, each bookstrap sample must be taken with replacement from the original sample\nis bootstrapping considered simulated data? no‚Äì simulated would be new people. this is reusing people\nlinear function\nBeer = \\(\\alpha\\) + \\(\\beta\\)(wings) | n=30\n\ntake sample n=30\ndo regression to find \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\nkeep track of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\ndo this 10,000 times\nplot distribution\n\nwe end up with a bootstrap distribution for \\(\\hat{\\beta}\\)"
  },
  {
    "objectID": "posts/week-11/tree-based.html",
    "href": "posts/week-11/tree-based.html",
    "title": "wk 11: tree based",
    "section": "",
    "text": "simple, yet powerful\ndefine a space by evaluating a tree\nvery explainable for simple models\nEvery observation that falls into a region will have the same prediction, and that prediction will be based on the observations in that region\nRegression: mean value Classification: most common value\n\n\ngreedy algorithm tree based regression and classification method:\nuse recursive binary splitting approach (looks at 2 things at a time)\n\\[\n\\sum_{i=1}^N\n\\]\nis this also kind of a shrinkage method?\nbig trees tend to overfit, so you try to grow a large tree‚Äì split until you have a minimum number points in each region, and then prune according to some criteria (cost-complexity pruning)\nuse k-fold cross validation to choose \\(\\alpha\\)\nclassification: looking at proportions, but will classify by majority class\n\n\n\nmisclassification error: \\(1-p_m\\)\n\ngini index:\ncross-entropy or deviance:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cap5765 notes",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "iz",
    "section": "",
    "text": "cheers,\nisabel zimmerman"
  }
]