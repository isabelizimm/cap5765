[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "isabel zimmerman",
    "section": "",
    "text": "Just posting my silly little notes about computational data analysis!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cap5765",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/CAP5765-week-2/wk2-cda-linear.html",
    "href": "posts/CAP5765-week-2/wk2-cda-linear.html",
    "title": "wk 2: linear regression",
    "section": "",
    "text": "Linear regression is a fabulous place to start with statistical learning. The math is (relatively) simple, but it can feel like there‚Äôs a lot of moving parts if you‚Äôre new to all this. In general, we want to know 3 things this week about linear regression: create the best line possible, assess the coefficients and know if we did a good job.\n\n\n\nThe formula for least squares is:\n\\[\nY \\approx \\beta_0 + \\beta_1X\n\\]\nSo we want to calculate out two things: \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope).\nHere are the formulas:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThis course involves a lot of math, and variables. It‚Äôs hard to keep them straight if you‚Äôre not used to looking at a lot of statistics.\nBars (\\(\\bar{x}\\)) are for means. Think of üòê being a mean face.\nHats (\\(\\hat{x}\\)) are for estimated numbers. Think of \\(\\hat{y}_0\\) as wearing a hat to cosplay as their best guess of what \\(y_0\\) looks like ü§† (sometimes y is not very good at cosplaying).\n\n\nThese equations really involve just plugging in numbers, which our program is mostly going to do, so let‚Äôs not be too concerned with this right now.\n\n\n\nRounding up a few key points:\n\nThe difference between \\(\\hat{y}_i\\) and \\(y_i\\) is \\(e_1\\). This is the distance between the actual data point and our model.\n\nOne way to know how well our coefficients might perform is finding the standard error of an estimator to see how it varies under repeated sampling.\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nStandard error has a reverse relation with number of training set.\nNoise in the data affects errors in all coefficients, and is directly proportional. (ie, 4x as much data will tend to reduce standard error by 2)\n\n\nResidual Standard Error = \\(\\sqrt{-\\frac{RSS}{n-2}}\\)\nLooking at RSE, it intuitively makes sense. We have the sum of squared residuals, so we need to square root to get back to just regular ol‚Äô residuals. We also need to divide by something to account for the degrees of freedom, thus \\(n-2\\). Interested in why we divide by 2? (tldr; residual degrees of freedom.)\nCONFIDENCE INTERVAL \\(\\hat{B_1} \\pm 2SE(\\hat{B_1})\\)\nWhat is a confidence interval tho? values that with 95% probability, the range will contain the true unknown value\nHere‚Äôs a thread to help understand:\n\n\nHey Stats folk, what's your 280 character definition of a confidence interval?ü§î\n\n‚Äî Chelsea Parlett-Pelleriti (@ChelseaParlett) March 13, 2018\n\n\n\n\n\n\\(H_0:\\) There is no relation between X and Y, that is \\(H_0:\\beta_1=0\\) \\(H_A:\\) There is some relationship between X and Y, that is \\(H_A:\\beta_1\\ne0\\)\nif \\(\\beta_1 = 0\\),\nto test null, we do a \\(t\\) statistic:\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nwhat is a p-value?\n\n\na small p-value is good‚Äì it means our model is accurate\na small p-value means we REJECT the NULL hypotheses, that is, there is SOME RELATION between X and Y\nhaving a large p-value‚Äì less likely there is a relation between X and Y\n\n\n\nRSE is used to either estimate the STANDARD ERROR of \\(\\beta_x\\) OR the accuracy of the overall model\n$ R^2 = = 1 - $ for how much variance is explained. A sanity check here: R^2 should always be [0,1].\nWe already know RSS and \\(TSS = \\sum_{i=1}^n(y_i-\\bar{y})\\)\n\n\n\n\\[\nSE(\\hat{\\beta}_1)^2 = \\frac{\\theta^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n\\]\n\\[\nSE(\\hat{\\beta}_0)^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2]}\n\\]\n\\[\nt = \\frac{\\hat{\\beta}_1-0}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nR^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\n\\]\n\\[\nRSE = \\sqrt{-\\frac{RSS}{n-2}}\n\\]"
  },
  {
    "objectID": "posts/CAP5765-week-1/wk1-cda-intro.html",
    "href": "posts/CAP5765-week-1/wk1-cda-intro.html",
    "title": "wk 1: vocab and linear algebra",
    "section": "",
    "text": "Learning primer:\nOn types of data:\n- Unstructured data does not follow any rhyme or reason\n- Semi-structured has some consitency, but no precise structure\n- Structured data has persistent order\nOn model outputs:\n- Quantitative data can be measured (numeric)\n- Qualitative data is descriptive (categorical) but qualitatative\nOn types of model:\n- Supervised learning is labeled data\n    - Some issues in supervised learning could be overfitting\n- Unsupervised learning is unlabeled data\n    - Some issues could be know\nmatricies: rectangular array, size (row, column) special <3 for SQUARE (n,n) matricies\n\\[\nA=[a_{ij}]\n\\]\n(the entry on row i and column j of matrix A)\nif \\(A\\) and \\(B\\) are the same size,\n\\(A + B\\)\n\\(B - B\\)\n\\(cA + rB\\) <- this is also called a linear combination of A and B\n\\[A = \\begin{bmatrix}\n1 & -1 & 1\\\\\n0 & 2 & 4\n\\end{bmatrix}\n\\]\n\nA = [[1,-1,1],[0,2,4]]\nB = [[2,-10,7], [3,8,6]]\n\nA+2*B # okay, this actually doesn't work\n\n[[1, -1, 1], [0, 2, 4], [2, -10, 7], [3, 8, 6], [2, -10, 7], [3, 8, 6]]\n\n\n\nmatrix1 <- matrix(c(3, 9, -1, 4, 2, 6), nrow = 2)\nmatrix2 <- matrix(c(5, 2, 0, 9, 3, 4), nrow = 2)\n\nmatrix1*matrix2 # OKAY THIS DOESNT WORK EITHER\n\n     [,1] [,2] [,3]\n[1,]   15    0    6\n[2,]   18   36   24\n\n\n\nMatrix multiplication\ndot product result is ALWAYS A NUMBER - use dot product to create matrix multiplication\nNotation:\n\nQuantitative output: Y\nQualitative output: G\nMatrix: A\n\nLinear Models: - you would use regression for numerical - you would use classification for\n\\[\ny = mx+b\n\\]\nwhere x = input y = output w = \\(\\begin{bmatrix} m \\\\ b \\end{bmatrix}\\)\ngoal is to make the line of best fit of the weights\nGiven input vector \\(X^T = (X_1, X_2, ..., X_p)\\) we predict \\(Y\\) by\n\\[\n\\hat{Y}= \\hat{B} + (X_1\\hat{B_1}+ X_1\\hat{B_2} + ... + X_p\\hat{B_p})\n\\]\n\\[\n\\hat{Y}= \\sum_{j=1}^PX_j\\hat{B_j}\n\\]\nwhere $ $ is the intercept.\nIf you have a vector \\(X=\\begin{bmatrix}X_1 \\\\ X_2 \\\\ ... \\\\ X_p\\end{bmatrix}\\)\nlinear model is\n\\[\nf(x) = X^T\\hat{B}\n\\]\nbut many different techniques to have flavors of linear modesl\n\\[\nError = y - \\hat{Y}\n\\]\nHow to tell if your model is doing well:\nResidual sum square of errors and least square we pick coefficients (weights), \\(\\hat{B}\\) to minimize the RSS\n\\[\nRSS(\\hat{B})= \\sum_{j=1}^N(y_j-x_j^T\\hat{B})^2\n\\]\nSimple linear regression:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are unknown\n\\[\n\\hat{y}=\\hat{B_0} + \\hat{B_1}x\n\\]\n\\[\n\\hat{y_i}=\\hat{B_0} + \\hat{B_1}x_i\n\\]\n(is the prediction of Y based on x_i)\n\\[\ne_i = y_i - \\hat{y_i}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nwhere \\(y_i\\) is actual and \\(\\hat{y_i}\\) is predicted\n\n\nThe least sqaure is\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{j=1}^N[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sum_{i=1}^N(x_i-\\bar{x})^2}\n\\]\n\\[\n\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\]"
  }
]